<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="ar">
  <head>
    <meta charset="utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>Multivariable Calculus &#8212; تعمّق في التعلّم العميق 0.7.1 documentation</title>

    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script type="text/javascript" src="../_static/d2l.js"></script>
    <script type="text/javascript" src="../_static/translations.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header "><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active">Multivariable Calculus</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai">
                  <i class="fas fa-user-graduate"></i>
                  Courses
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PDF
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.zip">
                  <i class="fas fa-download"></i>
                  All Notebooks
              </a>
          
              <a  class="mdl-navigation__link" href="https://discuss.mxnet.io">
                  <i class="fab fa-discourse"></i>
                  Discuss
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/d2l-ai/d2l-en">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh.d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  中文版
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="تعمّق في التعلّم العميق"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">الّرموز</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. مقدّمة</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. التمهيدات</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-math-and-keywords-translations/index.html">3. ملحق: ترجمة الألفاظ التقنية و الرياضية</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">المراجع</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="تعمّق في التعلّم العميق"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">الّرموز</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. مقدّمة</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. التمهيدات</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-math-and-keywords-translations/index.html">3. ملحق: ترجمة الألفاظ التقنية و الرياضية</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">المراجع</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="multivariable-calculus">
<span id="sec-multivariable-calculus"></span><h1>Multivariable Calculus<a class="headerlink" href="#multivariable-calculus" title="Permalink to this headline">¶</a><a href="https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.ipynb'); return false;"> <button style="float:right", id="colab" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab </button></a><div class="mdl-tooltip" data-mdl-for="colab"> Open the notebook in Colab</div></h1>
<p>Now that we have a fairly strong understanding of derivatives of a
function of a single variable, let’s return to our original question
where we were considering a loss function of potentially billions of
weights.</p>
<div class="section" id="higher-dimensional-differentiation">
<h2>Higher-Dimensional Differentiation<a class="headerlink" href="#higher-dimensional-differentiation" title="Permalink to this headline">¶</a></h2>
<p>What <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_single_variable_calculus</span></code> tells us is that if we
change a single one of these billions of weights leaving every other one
fixed, we know what will happen! This is nothing more than a function of
a single variable, so we can write</p>
<div class="math notranslate nohighlight" id="equation-eq-part-der">
<span class="eqno">()<a class="headerlink" href="#equation-eq-part-der" title="Permalink to this equation">¶</a></span>\[L(w_1+\epsilon_1, w_2, \ldots, w_N) \approx L(w_1, w_2, \ldots, w_N) + \epsilon_1 \frac{d}{dw_1} L(w_1, w_2, \ldots, w_N).\]</div>
<p>We will call the derivative in one variable while fixing the other the
<em>partial derivative</em>, and we will use the notation
<span class="math notranslate nohighlight">\(\frac{\partial}{\partial w_1}\)</span> for the derivative in
<a class="reference internal" href="#equation-eq-part-der">()</a>.</p>
<p>Now, let’s take this and change <span class="math notranslate nohighlight">\(w_2\)</span> a little bit to
<span class="math notranslate nohighlight">\(w_2 + \epsilon_2\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-0">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-0" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
L(w_1+\epsilon_1, w_2+\epsilon_2, \ldots, w_N) &amp; \approx L(w_1, w_2+\epsilon_2, \ldots, w_N) + \epsilon_1 \frac{\partial}{\partial w_1} L(w_1, w_2+\epsilon_2, \ldots, w_N) \\
&amp; \approx L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_2\frac{\partial}{\partial w_2} L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_1 \frac{\partial}{\partial w_1} L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_1\epsilon_2\frac{\partial}{\partial w_2}\frac{\partial}{\partial w_1} L(w_1, w_2, \ldots, w_N) \\
&amp; \approx L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_2\frac{\partial}{\partial w_2} L(w_1, w_2, \ldots, w_N) \\
&amp; \quad + \epsilon_1 \frac{\partial}{\partial w_1} L(w_1, w_2, \ldots, w_N).
\end{aligned}\end{split}\]</div>
<p>We have again used the idea that <span class="math notranslate nohighlight">\(\epsilon_1\epsilon_2\)</span> is a
higher order term that we can discard in the same way we could discard
<span class="math notranslate nohighlight">\(\epsilon^{2}\)</span> in the previous section, along with what we saw in
<a class="reference internal" href="#equation-eq-part-der">()</a>. By continuing in this manner, we may write that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-1">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-1" title="Permalink to this equation">¶</a></span>\[L(w_1+\epsilon_1, w_2+\epsilon_2, \ldots, w_N+\epsilon_N) \approx L(w_1, w_2, \ldots, w_N) + \sum_i \epsilon_i \frac{\partial}{\partial w_i} L(w_1, w_2, \ldots, w_N).\]</div>
<p>This may look like a mess, but we can make this more familiar by noting
that the sum on the right looks exactly like a dot product, so if we let</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-2">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-2" title="Permalink to this equation">¶</a></span>\[\boldsymbol{\epsilon} = [\epsilon_1, \ldots, \epsilon_N]^\top \; \text{and} \;
\nabla_{\mathbf{x}} L = \left[\frac{\partial L}{\partial x_1}, \ldots, \frac{\partial L}{\partial x_N}\right]^\top,\]</div>
<p>then</p>
<div class="math notranslate nohighlight" id="equation-eq-nabla-use">
<span class="eqno">()<a class="headerlink" href="#equation-eq-nabla-use" title="Permalink to this equation">¶</a></span>\[L(\mathbf{w} + \boldsymbol{\epsilon}) \approx L(\mathbf{w}) + \boldsymbol{\epsilon}\cdot \nabla_{\mathbf{w}} L(\mathbf{w}).\]</div>
<p>We will call the vector <span class="math notranslate nohighlight">\(\nabla_{\mathbf{w}} L\)</span> the <em>gradient</em> of
<span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>Equation <a class="reference internal" href="#equation-eq-nabla-use">()</a> is worth pondering for a moment. It has
exactly the format that we encountered in one dimension, just we have
converted everything to vectors and dot products. It allows us to tell
approximately how the function <span class="math notranslate nohighlight">\(L\)</span> will change given any
perturbation to the input. As we will see in the next section, this will
provide us with an important tool in understanding geometrically how we
can learn using information contained in the gradient.</p>
<p>But first, let’s see this approximation at work with an example. Suppose
that we are working with the function</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-3">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-3" title="Permalink to this equation">¶</a></span>\[f(x, y) = \log(e^x + e^y) \text{ with gradient } \nabla f (x, y) = \left[\frac{e^x}{e^x+e^y}, \frac{e^y}{e^x+e^y}\right].\]</div>
<p>If we look at a point like <span class="math notranslate nohighlight">\((0, \log(2))\)</span>, we see that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-4">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-4" title="Permalink to this equation">¶</a></span>\[f(x, y) = \log(3) \text{ with gradient } \nabla f (x, y) = \left[\frac{1}{3}, \frac{2}{3}\right].\]</div>
<p>Thus, if we want to approximate <span class="math notranslate nohighlight">\(f\)</span> at
<span class="math notranslate nohighlight">\((\epsilon_1, \log(2) + \epsilon_2)\)</span>, we see that we should have
the specific instance of <a class="reference internal" href="#equation-eq-nabla-use">()</a>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-5">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-5" title="Permalink to this equation">¶</a></span>\[f(\epsilon_1, \log(2) + \epsilon_2) \approx \log(3) + \frac{1}{3}\epsilon_1 + \frac{2}{3}\epsilon_2.\]</div>
<p>We can test this in code to see how good the approximation is.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">d2l</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">autograd</span><span class="p">,</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">))])</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03</span><span class="p">])</span>
<span class="n">grad_approx</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="n">epsilon</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_f</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="mi">0</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="s2">&quot;Approximation: </span><span class="si">{}</span><span class="s2">, True Value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grad_approx</span><span class="p">,</span> <span class="n">true_value</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;Approximation: 1.0819457, True Value: 1.0821242&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="geometry-of-gradients-and-gradient-descent">
<h2>Geometry of Gradients and Gradient Descent<a class="headerlink" href="#geometry-of-gradients-and-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>Consider the again <a class="reference internal" href="#equation-eq-nabla-use">()</a>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-6">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-6" title="Permalink to this equation">¶</a></span>\[L(\mathbf{w} + \boldsymbol{\epsilon}) \approx L(\mathbf{w}) + \boldsymbol{\epsilon}\cdot \nabla_{\mathbf{w}} L(\mathbf{w}).\]</div>
<p>Let’s suppose that I want to use this to help minimize our loss
<span class="math notranslate nohighlight">\(L\)</span>. Let’s understand geometrically the algorithm of gradient
descent first described in <a class="reference internal" href="../chapter_preliminaries/autograd.html#sec-autograd"><span class="std std-numref">Section 2.5</span></a>. What we will do is
the following:</p>
<ol class="arabic simple">
<li><p>Start with a random choice for the initial parameters
<span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</p></li>
<li><p>Find the direction <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> that makes <span class="math notranslate nohighlight">\(L\)</span> decrease
the most rapidly at <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</p></li>
<li><p>Take a small step in that direction:
<span class="math notranslate nohighlight">\(\mathbf{w} \rightarrow \mathbf{w} + \epsilon\mathbf{v}\)</span>.</p></li>
<li><p>Repeat.</p></li>
</ol>
<p>The only thing we do not know exactly how to do is to compute the vector
<span class="math notranslate nohighlight">\(\mathbf{v}\)</span> in the second step. We will call such a direction the
<em>direction of steepest descent</em>. Using the geometric understanding of
dot products from <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_geometry-linear-algebric-ops</span></code>, we see
that we can rewrite <a class="reference internal" href="#equation-eq-nabla-use">()</a> as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-7">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-7" title="Permalink to this equation">¶</a></span>\[L(\mathbf{w} + \mathbf{v}) \approx L(\mathbf{w}) + \mathbf{v}\cdot \nabla_{\mathbf{w}} L(\mathbf{w}) = \|\nabla_{\mathbf{w}} L(\mathbf{w})\|\cos(\theta).\]</div>
<p>Note that we have taken our direction to have length one for
convenience, and used <span class="math notranslate nohighlight">\(\theta\)</span> for the angle between
<span class="math notranslate nohighlight">\(\mathbf{v}\)</span> and <span class="math notranslate nohighlight">\(\nabla_{\mathbf{w}} L(\mathbf{w})\)</span>. If we
want to find the direction that decreases <span class="math notranslate nohighlight">\(L\)</span> as rapidly as
possible, we want to make this as expression as negative as possible.
The only way the direction we pick enters into this equation is through
<span class="math notranslate nohighlight">\(\cos(\theta)\)</span>, and thus we wish to make this cosine as negative
as possible. Now, recalling the shape of cosine, we can make this as
negative as possible by making <span class="math notranslate nohighlight">\(\cos(\theta) = -1\)</span> or equivalently
making the angle between the gradient and our chosen direction to be
<span class="math notranslate nohighlight">\(\pi\)</span> radians, or equivalently <span class="math notranslate nohighlight">\(180\)</span> degrees. The only way
to achieve this is to head in the exact opposite direction: pick
<span class="math notranslate nohighlight">\(\mathbf{v}\)</span> to point in the exact opposite direction to
<span class="math notranslate nohighlight">\(\nabla_{\mathbf{w}} L(\mathbf{w})\)</span>!</p>
<p>This brings us to one of the most important mathematical concepts in
machine learning: the direction of steepest decent points in the
direction of <span class="math notranslate nohighlight">\(-\nabla_{\mathbf{w}}L(\mathbf{w})\)</span>. Thus our
informal algorithm can be rewritten as follows.</p>
<ol class="arabic simple">
<li><p>Start with a random choice for the initial parameters
<span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\nabla_{\mathbf{w}} L(\mathbf{w})\)</span>.</p></li>
<li><p>Take a small step in the opposite of that direction:
<span class="math notranslate nohighlight">\(\mathbf{w} \rightarrow \mathbf{w} - \epsilon\nabla_{\mathbf{w}} L(\mathbf{w})\)</span>.</p></li>
<li><p>Repeat.</p></li>
</ol>
<p>This basic algorithm has been modified and adapted many ways by many
researchers, but the core concept remains the same in all of them. Use
the gradient to find the direction that decreases the loss as rapidly as
possible, and update the parameters to take a step in that direction.</p>
</div>
<div class="section" id="a-note-on-mathematical-optimization">
<h2>A Note on Mathematical Optimization<a class="headerlink" href="#a-note-on-mathematical-optimization" title="Permalink to this headline">¶</a></h2>
<p>Throughout this book, we focus squarely on numerical optimization
techniques for the practical reason that all functions we encounter in
the deep learning setting are too complex to minimize explicitly.</p>
<p>However, it is a useful exercise to consider what the geometric
understanding we obtained above tells us about optimizing functions
directly.</p>
<p>Suppose that we wish to find the value of <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> which
minimizes some function <span class="math notranslate nohighlight">\(L(\mathbf{x})\)</span>. Let’s suppose that
moreover someone gives us a value and tells us that it is the value that
minimizes <span class="math notranslate nohighlight">\(L\)</span>. Is there anything we can check to see if their
answer is even plausible?</p>
<p>Again consider <a class="reference internal" href="#equation-eq-nabla-use">()</a>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-8">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-8" title="Permalink to this equation">¶</a></span>\[L(\mathbf{x}_0 + \boldsymbol{\epsilon}) \approx L(\mathbf{x}_0) + \boldsymbol{\epsilon}\cdot \nabla_{\mathbf{x}} L(\mathbf{x}_0).\]</div>
<p>If the gradient is not zero, we know that we can take a step in the
direction <span class="math notranslate nohighlight">\(-\epsilon \nabla_{\mathbf{x}} L(\mathbf{x}_0)\)</span> to find
a value of <span class="math notranslate nohighlight">\(L\)</span> that is smaller. Thus, if we truly are at a
minimum, this cannot be the case! We can conclude that if
<span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> is a minimum, then
<span class="math notranslate nohighlight">\(\nabla_{\mathbf{x}} L(\mathbf{x}_0 = 0\)</span>. We call points with
<span class="math notranslate nohighlight">\(\nabla_{\mathbf{x}} L(\mathbf{x}_0 = 0\)</span> <em>critical points</em>.</p>
<p>This is nice, because in some rare settings, we <em>can</em> explicitly find
all the points where the gradient is zero, and find the one with the
smallest value.</p>
<p>For a concrete example, consider the function</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-9">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-9" title="Permalink to this equation">¶</a></span>\[f(x) = 3x^4 - 4x^3 -12x^2.\]</div>
<p>This function has derivative</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-10">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-10" title="Permalink to this equation">¶</a></span>\[\frac{df}{dx} = 12x^3 - 12x^2 -24x = 12x(x-2)(x+1).\]</div>
<p>The only possible location of minima are at <span class="math notranslate nohighlight">\(x = -1, 0, 2\)</span>, where
the function takes the values <span class="math notranslate nohighlight">\(-5,0, -32\)</span> respectively, and thus
we can conclude that we minimize our function when <span class="math notranslate nohighlight">\(x = 2\)</span>. A
quick plot confirms this.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">12</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_multivariable-calculus_835c21_3_0.svg" src="../_images/output_multivariable-calculus_835c21_3_0.svg" /></div>
<p>This highlights an important fact to know when working either
theoretically or numerically: the only possible points where we can
minimize (or maximize) a function will have gradient equal to zero,
however, not every point with gradient zero is the true <em>global</em> minimum
(or maximum).</p>
</div>
<div class="section" id="multivariate-chain-rule">
<h2>Multivariate Chain Rule<a class="headerlink" href="#multivariate-chain-rule" title="Permalink to this headline">¶</a></h2>
<p>Let’s suppose that we have a function of four variables
(<span class="math notranslate nohighlight">\(w, x, y\)</span>, and <span class="math notranslate nohighlight">\(z\)</span>) which we can make by composing many
terms:</p>
<div class="math notranslate nohighlight" id="equation-eq-multi-func-def">
<span class="eqno">()<a class="headerlink" href="#equation-eq-multi-func-def" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}f(u, v) &amp; = (u+v)^{2} \\u(a, b) &amp; = (a+b)^{2}, \qquad v(a, b) = (a-b)^{2}, \\a(w, x, y, z) &amp; = (w+x+y+z)^{2}, \qquad b(w, x, y, z) = (w+x-y-z)^2.\end{aligned}\end{split}\]</div>
<p>Such chains of equations are common when working with neural networks,
so trying to understand how to compute gradients of such functions is
key. We can start to see visual hints of this connection in
<code class="xref std std-numref docutils literal notranslate"><span class="pre">fig_chain-1</span></code> if we take a look at what variables directly
relate to one another.</p>
<div class="figure align-default" id="id2">
<span id="fig-chain-1"></span><img alt="../_images/ChainNet1.svg" src="../_images/ChainNet1.svg" /><p class="caption"><span class="caption-text">The function relations above where nodes represent values and edges
show functional dependence.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>Nothing stops us from just composing everything from
<a class="reference internal" href="#equation-eq-multi-func-def">()</a> and writing out that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-11">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-11" title="Permalink to this equation">¶</a></span>\[f(w, x, y, z) = \left(\left((w+x+y+z)^2+(w+x-y-z)^2\right)^2+\left((w+x+y+z)^2-(w+x-y-z)^2\right)^2\right)^2.\]</div>
<p>We may then take the derivative by just using single variable
derivatives, but if we did that we would quickly find ourself swamped
with terms, many of which are repeats! Indeed, one can see that, for
instance:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-12">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-12" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial w} &amp; = 2 \left(2 \left(2 (w + x + y + z) - 2 (w + x - y - z)\right) \left((w + x + y + z)^{2}- (w + x - y - z)^{2}\right) + \right.\\
&amp; \left. \quad 2 \left(2 (w + x - y - z) + 2 (w + x + y + z)\right) \left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\right)\right) \times \\
&amp; \quad \left(\left((w + x + y + z)^{2}- (w + x - y - z)^2\right)^{2}+ \left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\right)^{2}\right).
\end{aligned}\end{split}\]</div>
<p>If we then also wanted to compute <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span>,
we would end up with a similar equation again with many repeated terms,
and many <em>shared</em> repeated terms between the two derivatives. This
represents a massive quantity of wasted work, and if we needed to
compute derivatives this way, the whole deep learning revolution would
have stalled out before it began!</p>
<p>Let’s break up the problem. We will start by trying to understand how
<span class="math notranslate nohighlight">\(f\)</span> changes when we change <span class="math notranslate nohighlight">\(a\)</span>, essentially assuming that
<span class="math notranslate nohighlight">\(w, x, y\)</span>, and <span class="math notranslate nohighlight">\(z\)</span> all do not exist. We will reason as we
did back when we worked with the gradient for the first time. Let’s take
<span class="math notranslate nohighlight">\(a\)</span> and add a small amount <span class="math notranslate nohighlight">\(\epsilon\)</span> to it.</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-13">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-13" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
&amp; f(u(a+\epsilon, b), v(a+\epsilon, b)) \\
\approx &amp; f\left(u(a, b) + \epsilon\frac{\partial u}{\partial a}(a, b), v(a, b) + \epsilon\frac{\partial v}{\partial a}(a, b)\right) \\
\approx &amp; f(u(a, b), v(a, b)) + \epsilon\left[\frac{\partial f}{\partial u}(u(a, b), v(a, b))\frac{\partial u}{\partial a}(a, b) + \frac{\partial f}{\partial v}(u(a, b), v(a, b))\frac{\partial v}{\partial a}(a, b)\right].
\end{aligned}\end{split}\]</div>
<p>The first line follows from the definition of partial derivative, and
the second follows from the definition of gradient. It is notationally
burdensome to track exactly where we evaluate every derivative, as in
the expression <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial u}(u(a, b), v(a, b))\)</span>,
so we often abbreviate this to the much more memorable</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-14">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-14" title="Permalink to this equation">¶</a></span>\[\frac{\partial f}{\partial a} = \frac{\partial f}{\partial u}\frac{\partial u}{\partial a}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial a}.\]</div>
<p>It is useful to think about the meaning of the process. We are trying to
understand how a function of the form <span class="math notranslate nohighlight">\(f(u(a, b), v(a, b))\)</span>
changes its value with a change in <span class="math notranslate nohighlight">\(a\)</span>. There are two pathways
this can occur: there is the pathway where
<span class="math notranslate nohighlight">\(a \rightarrow u \rightarrow f\)</span> and where
<span class="math notranslate nohighlight">\(a \rightarrow v \rightarrow f\)</span>. We can compute both of these
contributions via the chain rule:
<span class="math notranslate nohighlight">\(\frac{\partial w}{\partial u} \cdot \frac{\partial u}{\partial x}\)</span>
and
<span class="math notranslate nohighlight">\(\frac{\partial w}{\partial v} \cdot \frac{\partial v}{\partial x}\)</span>
respectively, and added up.</p>
<p>Imagine we have a different network of functions where the functions on
the right depend on those they are connected to on the left as is shown
in <code class="xref std std-numref docutils literal notranslate"><span class="pre">fig_chain-2</span></code>.</p>
<div class="figure align-default" id="id3">
<span id="fig-chain-2"></span><img alt="../_images/ChainNet2.svg" src="../_images/ChainNet2.svg" /><p class="caption"><span class="caption-text">Another more subtle example of the chain rule.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>To compute something like <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y}\)</span>, we need
to sum over all (in this case <span class="math notranslate nohighlight">\(3\)</span>) paths from <span class="math notranslate nohighlight">\(y\)</span> to
<span class="math notranslate nohighlight">\(f\)</span> giving</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-15">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-15" title="Permalink to this equation">¶</a></span>\[\frac{\partial f}{\partial y} = \frac{\partial f}{\partial a} \frac{\partial a}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial b} \frac{\partial b}{\partial v} \frac{\partial v}{\partial y}.\]</div>
<p>Understanding the chain rule in this way will pay great dividends when
trying to understand how gradients flow through networks, and why
various architectural choices like those in LSTMs (<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_lstm</span></code>)
or residual layers (<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_resnet</span></code>) can help shape the learning
process by controlling gradient flow.</p>
</div>
<div class="section" id="the-backpropagation-algorithm">
<h2>The Backpropagation Algorithm<a class="headerlink" href="#the-backpropagation-algorithm" title="Permalink to this headline">¶</a></h2>
<p>Let’s return to the example of <a class="reference internal" href="#equation-eq-multi-func-def">()</a> the previous
section where</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-16">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-16" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
f(u, v) &amp; = (u+v)^{2} \\
u(a, b) &amp; = (a+b)^{2}, \qquad v(a, b) = (a-b)^{2}, \\
a(w, x, y, z) &amp; = (w+x+y+z)^{2}, \qquad b(w, x, y, z) = (w+x-y-z)^2.
\end{aligned}\end{split}\]</div>
<p>If we want to compute say <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial w}\)</span> we may
apply the multi-variate chain rule to see:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-17">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-17" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial w} &amp; = \frac{\partial f}{\partial u}\frac{\partial u}{\partial w} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial w}, \\
\frac{\partial u}{\partial w} &amp; = \frac{\partial u}{\partial a}\frac{\partial a}{\partial w}+\frac{\partial u}{\partial b}\frac{\partial b}{\partial w}, \\
\frac{\partial v}{\partial w} &amp; = \frac{\partial v}{\partial a}\frac{\partial a}{\partial w}+\frac{\partial v}{\partial b}\frac{\partial b}{\partial w}.
\end{aligned}\end{split}\]</div>
<p>Let’s try using this decomposition to compute
<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial w}\)</span>. Notice that all we need here are
the various single step partials:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-18">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-18" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial u} = 2(u+v), &amp; \quad\frac{\partial f}{\partial v} = 2(u+v), \\
\frac{\partial u}{\partial a} = 2(a+b), &amp; \quad\frac{\partial u}{\partial b} = 2(a+b), \\
\frac{\partial v}{\partial a} = 2(a-b), &amp; \quad\frac{\partial v}{\partial b} = -2(a-b), \\
\frac{\partial a}{\partial w} = 2(w+x+y+z), &amp; \quad\frac{\partial b}{\partial w} = 2(w+x-y-z).
\end{aligned}\end{split}\]</div>
<p>If we write this out into code this becomes a fairly manageable
expression.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the value of the function from inputs to outputs</span>
<span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    f at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">f</span><span class="p">))</span>

<span class="c1"># Compute the single step partials</span>
<span class="n">df_du</span><span class="p">,</span> <span class="n">df_dv</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
<span class="n">du_da</span><span class="p">,</span> <span class="n">du_db</span><span class="p">,</span> <span class="n">dv_da</span><span class="p">,</span> <span class="n">dv_db</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="n">da_dw</span><span class="p">,</span> <span class="n">db_dw</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>

<span class="c1"># Compute the final result from inputs to outputs</span>
<span class="n">du_dw</span><span class="p">,</span> <span class="n">dv_dw</span> <span class="o">=</span> <span class="n">du_da</span><span class="o">*</span><span class="n">da_dw</span> <span class="o">+</span> <span class="n">du_db</span><span class="o">*</span><span class="n">db_dw</span><span class="p">,</span> <span class="n">dv_da</span><span class="o">*</span><span class="n">da_dw</span> <span class="o">+</span> <span class="n">dv_db</span><span class="o">*</span><span class="n">db_dw</span>
<span class="n">df_dw</span> <span class="o">=</span> <span class="n">df_du</span><span class="o">*</span><span class="n">du_dw</span> <span class="o">+</span> <span class="n">df_dv</span><span class="o">*</span><span class="n">dv_dw</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df/dw at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">df_dw</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">f</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="mi">1024</span>
<span class="n">df</span><span class="o">/</span><span class="n">dw</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
</pre></div>
</div>
<p>However, note that this still does not make it easy to compute something
like <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span>. The reason for that is the
<em>way</em> we chose to apply the chain rule. If we look at what we did above,
we always kept <span class="math notranslate nohighlight">\(\partial w\)</span> in the denominator when we could. In
this way, we chose to apply the chain rule seeing how <span class="math notranslate nohighlight">\(w\)</span> changed
every other variable. If that is what we wanted, this would be a good
idea. However, think back to our motivation from deep learning: we want
to see how every parameter changes the <em>loss</em>. In essence, we want to
apply the chain rule keeping <span class="math notranslate nohighlight">\(\partial f\)</span> in the numerator
whenever we can!</p>
<p>To be more explicit, note that we can write</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-19">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-19" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial w} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial w} + \frac{\partial f}{\partial b}\frac{\partial b}{\partial w}, \\
\frac{\partial f}{\partial a} &amp; = \frac{\partial f}{\partial u}\frac{\partial u}{\partial a}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial a}, \\
\frac{\partial f}{\partial b} &amp; = \frac{\partial f}{\partial u}\frac{\partial u}{\partial b}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial b}.
\end{aligned}\end{split}\]</div>
<p>Note that this application of the chain rule has us explicitly compute
<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial u}, \frac{\partial f}{\partial u}, \frac{\partial f}{\partial u}, \frac{\partial f}{\partial u}, \; \text{and} \; \frac{\partial f}{\partial u}\)</span>.
Nothing stops us from also including the equations:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-20">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-20" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{\partial f}{\partial x} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial x} + \frac{\partial f}{\partial b}\frac{\partial b}{\partial x}, \\
\frac{\partial f}{\partial y} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial y}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial y}, \\
\frac{\partial f}{\partial z} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial z}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial z}.
\end{aligned}\end{split}\]</div>
<p>and then keeping track of how <span class="math notranslate nohighlight">\(f\)</span> changes when we change <em>any</em>
node in the entire network. Let’s implement it.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the value of the function from inputs to outputs</span>
<span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    f at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">f</span><span class="p">))</span>

<span class="c1"># Compute the derivative using the decomposition above</span>
<span class="c1"># First compute the single step partials</span>
<span class="n">df_du</span><span class="p">,</span> <span class="n">df_dv</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
<span class="n">du_da</span><span class="p">,</span> <span class="n">du_db</span><span class="p">,</span> <span class="n">dv_da</span><span class="p">,</span> <span class="n">dv_db</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="n">da_dw</span><span class="p">,</span> <span class="n">db_dw</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>
<span class="n">da_dx</span><span class="p">,</span> <span class="n">db_dx</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>
<span class="n">da_dy</span><span class="p">,</span> <span class="n">db_dy</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>
<span class="n">da_dz</span><span class="p">,</span> <span class="n">db_dz</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>

<span class="c1"># Now compute how f changes when we change any value from output to input</span>
<span class="n">df_da</span><span class="p">,</span> <span class="n">df_db</span> <span class="o">=</span> <span class="n">df_du</span><span class="o">*</span><span class="n">du_da</span> <span class="o">+</span> <span class="n">df_dv</span><span class="o">*</span><span class="n">dv_da</span><span class="p">,</span> <span class="n">df_du</span><span class="o">*</span><span class="n">du_db</span> <span class="o">+</span> <span class="n">df_dv</span><span class="o">*</span><span class="n">dv_db</span>
<span class="n">df_dw</span><span class="p">,</span> <span class="n">df_dx</span> <span class="o">=</span> <span class="n">df_da</span><span class="o">*</span><span class="n">da_dw</span> <span class="o">+</span> <span class="n">df_db</span><span class="o">*</span><span class="n">db_dw</span><span class="p">,</span> <span class="n">df_da</span><span class="o">*</span><span class="n">da_dx</span> <span class="o">+</span> <span class="n">df_db</span><span class="o">*</span><span class="n">db_dx</span>
<span class="n">df_dy</span><span class="p">,</span> <span class="n">df_dz</span> <span class="o">=</span> <span class="n">df_da</span><span class="o">*</span><span class="n">da_dy</span> <span class="o">+</span> <span class="n">df_db</span><span class="o">*</span><span class="n">db_dy</span><span class="p">,</span> <span class="n">df_da</span><span class="o">*</span><span class="n">da_dz</span> <span class="o">+</span> <span class="n">df_db</span><span class="o">*</span><span class="n">db_dz</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df/dw at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">df_dw</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df/dx at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">df_dx</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df/dy at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">df_dy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df/dz at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">df_dz</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">f</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="mi">1024</span>
<span class="n">df</span><span class="o">/</span><span class="n">dw</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
<span class="n">df</span><span class="o">/</span><span class="n">dx</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
<span class="n">df</span><span class="o">/</span><span class="n">dy</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
<span class="n">df</span><span class="o">/</span><span class="n">dz</span> <span class="n">at</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="ow">is</span> <span class="o">-</span><span class="mi">4096</span>
</pre></div>
</div>
<p>The fact that we compute derivatives from <span class="math notranslate nohighlight">\(f\)</span> back towards the
inputs rather than from the inputs forward to the outputs (as we did in
the first code snippet above) is what gives this algorithm its name:
<em>backpropagation</em>. Note that there are two steps: 1. Compute the value
of the function, and the single step partials from front to back. While
not done above, this can be combined into a single <em>forward pass</em>. 2.
Compute the gradient of <span class="math notranslate nohighlight">\(f\)</span> from back to front. We call this the
<em>backwards pass</em>.</p>
<p>This is precisely what every deep learning algorithm implements to allow
the computation of the gradient of the loss with respect to every weight
in the network at one pass. It is an astonishing fact that we have such
a decomposition.</p>
<p>To see how MXNet has encapsulated this, let’s take a quick look at this
example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize as ndarrays, then attach gradients</span>
<span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">w</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>
<span class="n">x</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>
<span class="n">z</span><span class="o">.</span><span class="n">attach_grad</span><span class="p">()</span>

<span class="c1"># Do the computation like usual, tracking gradients</span>
<span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Execute backward pass</span>
<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df/dw at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df/dx at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df/dy at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df/dz at </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">grad</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">/</span><span class="n">dw</span> <span class="n">at</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">4096.0</span>
<span class="n">df</span><span class="o">/</span><span class="n">dx</span> <span class="n">at</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">4096.0</span>
<span class="n">df</span><span class="o">/</span><span class="n">dy</span> <span class="n">at</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">4096.0</span>
<span class="n">df</span><span class="o">/</span><span class="n">dz</span> <span class="n">at</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="ow">is</span> <span class="o">-</span><span class="mf">4096.0</span>
</pre></div>
</div>
<p>All of what we did above can be done automatically by calling
<code class="docutils literal notranslate"><span class="pre">f.backwards()</span></code>.</p>
</div>
<div class="section" id="hessians">
<h2>Hessians<a class="headerlink" href="#hessians" title="Permalink to this headline">¶</a></h2>
<p>As with single variable calculus, it is useful to consider higher-order
derivatives in order to get a handle on how we can obtain a better
approximation to a function than using the gradient alone.</p>
<p>There is one immediate problem one encounters when working with higher
order derivatives of functions of several variables, and that is there
are a large number of them. If we have a function
<span class="math notranslate nohighlight">\(f(x_1, \ldots, x_n)\)</span> of <span class="math notranslate nohighlight">\(n\)</span> variables, then we can take
<span class="math notranslate nohighlight">\(n^{2}\)</span> many second derivatives, namely for any choice of
<span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-21">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-21" title="Permalink to this equation">¶</a></span>\[\frac{d^2f}{dx_idx_j} = \frac{d}{dx_i}\left(\frac{d}{dx_j}f\right).\]</div>
<p>This is traditionally assembled into a matrix called the <em>Hessian</em>:</p>
<div class="math notranslate nohighlight" id="equation-eq-hess-def">
<span class="eqno">()<a class="headerlink" href="#equation-eq-hess-def" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{H}_f = \begin{bmatrix} \frac{d^2f}{dx_1dx_1} &amp; \cdots &amp; \frac{d^2f}{dx_1dx_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{d^2f}{dx_ndx_1} &amp; \cdots &amp; \frac{d^2f}{dx_ndx_n} \\ \end{bmatrix}.\end{split}\]</div>
<p>Not every entry of this matrix is independent. Indeed, we can show that
as long as both <em>mixed partials</em> (partial derivatives with respect to
more than one variable) exist and are continuous, we can say that for
any <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(j\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-22">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-22" title="Permalink to this equation">¶</a></span>\[\frac{d^2f}{dx_idx_j} = \frac{d^2f}{dx_jdx_i}.\]</div>
<p>This follows by considering first perturbing a function in the direction
of <span class="math notranslate nohighlight">\(x_i\)</span>, and then perturbing it in <span class="math notranslate nohighlight">\(x_j\)</span> and then comparing
the result of that with what happens if we perturb first <span class="math notranslate nohighlight">\(x_j\)</span> and
then <span class="math notranslate nohighlight">\(x_i\)</span>, with the knowledge that both of these orders lead to
the same final change in the output of <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>As with single variables, we can use these derivatives to get a far
better idea of how the function behaves near a point. In particular, we
can use it to find the best fitting quadratic near a point
<span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, as we saw in a single variable.</p>
<p>Let’s see an example. Suppose that
<span class="math notranslate nohighlight">\(f(x_1, x_2) = a + b_1x_1 + b_2x_2 + c_{11}x_1^{2} + c_{12}x_1x_2 + c_{22}x_2^{2}\)</span>.
This is the general form for a quadratic in two variables. If we look at
the value of the function, its gradient, and its Hessian
<a class="reference internal" href="#equation-eq-hess-def">()</a>, all at the point zero:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-23">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-23" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
f(0,0) &amp; = a, \\
\nabla f (0,0) &amp; = \begin{bmatrix}b_1 \\ b_2\end{bmatrix}, \\
\mathbf{H} f (0,0) &amp; = \begin{bmatrix}2 c_{11} &amp; c_{12} \\ c_{12} &amp; 2c_{22}\end{bmatrix}.
\end{aligned}\end{split}\]</div>
<p>If we from this, we see we can get our original polynomial back by
saying</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-24">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-24" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x}) = f(0) + \nabla f (0) \cdot \mathbf{x} + \frac{1}{2}\mathbf{x}^\top \mathbf{H} f (0) \mathbf{x}.\]</div>
<p>In general, if we computed this expansion any point
<span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, we see that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-25">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-25" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x}) = f(\mathbf{x}_0) + \nabla f (\mathbf{x}_0) \cdot (\mathbf{x}-\mathbf{x}_0) + \frac{1}{2}(\mathbf{x}-\mathbf{x}_0)^\top \mathbf{H} f (\mathbf{x}_0) (\mathbf{x}-\mathbf{x}_0).\]</div>
<p>This works for any dimensional input, and provides the best
approximating quadratic to any function at a point. To give an example,
let’s plot the function</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-26">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-26" title="Permalink to this equation">¶</a></span>\[f(x, y) = xe^{-x^2-y^2}.\]</div>
<p>One can compute that the gradient and Hessian are</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-27">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-27" title="Permalink to this equation">¶</a></span>\[\begin{split}\nabla f(x, y) = e^{-x^2-y^2}\begin{pmatrix}1-2x^2 \\ -2xy\end{pmatrix} \; \text{and} \; \mathbf{H}f(x, y) = e^{-x^2-y^2}\begin{pmatrix} 4x^3 - 6x &amp; 4x^2y - 2y \\ 4x^2y-2y &amp;4xy^2-2x\end{pmatrix}.\end{split}\]</div>
<p>And thus, with a little algebra, see that the approximating quadratic at
<span class="math notranslate nohighlight">\([-1,0]^\top\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-28">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-28" title="Permalink to this equation">¶</a></span>\[f(x, y) \approx e^{-1}\left(-1 - (x+1) +2(x+1)^2+2y^2\right).\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct grid and compute function</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span>
                   <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span> <span class="n">indexing</span><span class="o">=</span><span class="s1">&#39;ij&#39;</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Compute gradient and Hessian at (1, 0)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot function</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s1">&#39;rstride&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;cstride&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s1">&#39;rstride&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;cstride&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_multivariable-calculus_835c21_11_0.svg" src="../_images/output_multivariable-calculus_835c21_11_0.svg" /></div>
<p>This forms the basis for Newton’s Algorithm discussed in
<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_gd</span></code>, where we perform numerical optimization iteratively
finding the best fitting quadratic, and then exactly minimizing that
quadratic.</p>
</div>
<div class="section" id="a-little-matrix-calculus">
<h2>A Little Matrix Calculus<a class="headerlink" href="#a-little-matrix-calculus" title="Permalink to this headline">¶</a></h2>
<p>Derivatives of functions involving matrices turn out to be particularly
nice. This section can become notationally heavy, so may be skipped in a
first reading, but it is useful to know how derivatives of functions
involving common matrix operations are often much cleaner than one might
initially anticipate, particularly given how central matrix operations
are to deep learning applications.</p>
<p>Let’s begin with an example. Suppose that we have some fixed row vector
<span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, and we want to take the product function
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \boldsymbol{\beta}\mathbf{x}\)</span>, and understand how
the dot product changes when we change <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>A bit of notation that will be useful when working with matrix
derivatives in ML is called the <em>denominator layout matrix derivative</em>
where we assemble our partial derivatives into the shape of whatever
vector, matrix, or tensor is in the denominator of the differential. In
this case, we will write</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-29">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-29" title="Permalink to this equation">¶</a></span>\[\begin{split}\frac{df}{d\mathbf{x}} = \begin{bmatrix}
\frac{df}{dx_1} \\
\vdots \\
\frac{df}{dx_n}
\end{bmatrix}.\end{split}\]</div>
<p>where we matched the shape of the column vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>If we write out our function into components this is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-30">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-30" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x}) = \sum_{i = 1}^{n} \beta_ix_i = \beta_1x_1 + \cdots + \beta_nx_n.\]</div>
<p>If we now take the partial derivative with respect to say
<span class="math notranslate nohighlight">\(\beta_1\)</span>, note that everything is zero but the first term, which
is just <span class="math notranslate nohighlight">\(x_1\)</span> multiplied by <span class="math notranslate nohighlight">\(\beta_1\)</span>, so the we obtain that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-31">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-31" title="Permalink to this equation">¶</a></span>\[\frac{df}{dx_1} = \beta_1,\]</div>
<p>or more generally that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-32">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-32" title="Permalink to this equation">¶</a></span>\[\frac{df}{dx_i} = \beta_i.\]</div>
<p>We can now reassemble this into a matrix to see</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-33">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-33" title="Permalink to this equation">¶</a></span>\[\begin{split}\frac{df}{d\mathbf{x}} = \begin{bmatrix}
\frac{df}{dx_1} \\
\vdots \\
\frac{df}{dx_n}
\end{bmatrix} = \begin{bmatrix}
\beta_1 \\
\vdots \\
\beta_n
\end{bmatrix} = \boldsymbol{\beta}^\top.\end{split}\]</div>
<p>This illustrates a few factors about matrix calculus that we will often
counter throughout this section:</p>
<ul class="simple">
<li><p>First, The computations will get rather involved.</p></li>
<li><p>Second, The final results are much cleaner than the intermediate
process, and will always look similar to the single variable case. In
this case, note that <span class="math notranslate nohighlight">\(\frac{d}{dx}(bx) = b\)</span> and
<span class="math notranslate nohighlight">\(\frac{d}{d\mathbf{x}} (\boldsymbol{\beta}\mathbf{x}) = \boldsymbol{\beta}^\top\)</span>
are both similar.</p></li>
<li><p>Third, transposes can often appear seemingly from nowhere. The core
reason for this is the convention that we match the shape of the
denominator, thus when we multiply matrices, we will need to take
transposes to match back to the shape of the original term.</p></li>
</ul>
<p>To keep building intuition, let’s try a computation that is a little
harder. Suppose that we have a column vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, and a
square matrix <span class="math notranslate nohighlight">\(A\)</span> and we want to compute</p>
<div class="math notranslate nohighlight" id="equation-eq-mat-goal-1">
<span class="eqno">()<a class="headerlink" href="#equation-eq-mat-goal-1" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x}).\]</div>
<p>To drive towards easier to manipulate notation, let’s consider this
problem using Einstein notation. In this case we can write the function
as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-34">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-34" title="Permalink to this equation">¶</a></span>\[\mathbf{x}^\top A \mathbf{x} = x_ia_{ij}x_j.\]</div>
<p>To compute our derivative, we need to understand for every <span class="math notranslate nohighlight">\(k\)</span>,
what the value of</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-35">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-35" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}(\mathbf{x}^\top A \mathbf{x}) = \frac{d}{dx_k}x_ia_{ij}x_j.\]</div>
<p>By the product rule, this is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-36">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-36" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}x_ia_{ij}x_j = \frac{dx_i}{dx_k}a_{ij}x_j + x_ia_{ij}\frac{dx_j}{dx_k}.\]</div>
<p>For a term like <span class="math notranslate nohighlight">\(\frac{dx_i}{dx_k}\)</span>, it is not hard to see that
this is one when <span class="math notranslate nohighlight">\(i=k\)</span> and zero otherwise. This means that every
term where <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(k\)</span> are different vanish from this sum,
so the only terms that remain in that first sum are the ones where
<span class="math notranslate nohighlight">\(i=k\)</span>. The same reasoning holds for the second term where we need
<span class="math notranslate nohighlight">\(j=k\)</span>. This gives</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-37">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-37" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}x_ia_{ij}x_j = a_{kj}x_j + x_ia_{ik}.\]</div>
<p>Now, the names of the indices in Einstein notation are arbitrary—the
fact that <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are different is immaterial to this
computation at this point, so we can re-index so that they both use
<span class="math notranslate nohighlight">\(i\)</span> to see that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-38">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-38" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}x_ia_{ij}x_j = a_{ki}x_i + x_ia_{ik} = (a_{ki} + a_{ik})x_i.\]</div>
<p>Now, here is where we start to need some practice to go further. Let’s
try and identify this outcome in terms of matrix operations.
<span class="math notranslate nohighlight">\(a_{ki} + a_{ik}\)</span> is the <span class="math notranslate nohighlight">\(k, i\)</span>-th component of
<span class="math notranslate nohighlight">\(\mathbf{A} + \mathbf{A}^\top\)</span>. This gives</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-39">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-39" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_k}x_ia_{ij}x_j = [\mathbf{A} + \mathbf{A}^\top]_{ki}x_i.\]</div>
<p>Similarly, this term is now the product of the matrix
<span class="math notranslate nohighlight">\(\mathbf{A} + \mathbf{A}^\top\)</span> by the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>,
so we see that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-40">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-40" title="Permalink to this equation">¶</a></span>\[\left[\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x})\right]_k = \frac{d}{dx_k}x_ia_{ij}x_j = [(\mathbf{A} + \mathbf{A}^\top)\mathbf{x}]_k.\]</div>
<p>Thus, we see that the <span class="math notranslate nohighlight">\(k\)</span>-th entry of the desired derivative from
<a class="reference internal" href="#equation-eq-mat-goal-1">()</a> is just the <span class="math notranslate nohighlight">\(k\)</span>-th entry of the vector on
the right, and thus the two are the same. Thus yields</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-41">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-41" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x}) = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}.\]</div>
<p>This required significantly more work than our last one, but the final
result is small. More than that, consider the following computation for
traditional single variable derivatives:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-42">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-42" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx}(xax) = \frac{dx}{dx}ax + xa\frac{dx}{dx} = (a+a)x.\]</div>
<p>Equivalently <span class="math notranslate nohighlight">\(\frac{d}{dx}(ax^2) = 2ax = (a+a)x\)</span>. Again, we get a
result that looks rather like the single variable result but with a
transpose tossed in.</p>
<p>At this point, the pattern should be looking rather suspicious, so let’s
try to figure out why. When we take matrix derivatives like this, let’s
first assume that the expression we get will be another matrix
expression: an expression we can write it in terms of products and sums
of matrices and their transposes. If such an expression exists, it will
need to be true for all matrices. In particular, it will need to be true
of <span class="math notranslate nohighlight">\(1 \times 1\)</span> matrices, in which case the matrix product is just
the product of the numbers, the matrix sum is just the sum, and the
transpose does nothing at all! In other words, whatever expression we
get <em>must</em> match the single variable expression. This means that, with
some practice, one can often guess matrix derivatives just by knowing
what the associated single variable expression must look like!</p>
<p>Let’s try this out. Suppose that <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a
<span class="math notranslate nohighlight">\(n \times m\)</span> matrix, <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> is an <span class="math notranslate nohighlight">\(n \times r\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> is an <span class="math notranslate nohighlight">\(r \times m\)</span>. Let’s try to compute</p>
<div class="math notranslate nohighlight" id="equation-eq-mat-goal-2">
<span class="eqno">()<a class="headerlink" href="#equation-eq-mat-goal-2" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2} = \;?\]</div>
<p>This computation is important in an area called matrix factorization.
For us, however, it is just a derivative to compute. Let’s try to
imaging what this would be for <span class="math notranslate nohighlight">\(1\times1\)</span> matrices. In that case,
we get the expression</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-43">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-43" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv} (x-uv)^{2}= 2(x-uv)u,\]</div>
<p>where, the derivative is rather standard. If we try to convert this back
into a matrix expression we get</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-44">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-44" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= 2(\mathbf{X} - \mathbf{U}\mathbf{V})\mathbf{U}.\]</div>
<p>However, if we look at this it does not quite work. Recall that
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is <span class="math notranslate nohighlight">\(n \times m\)</span>, as is
<span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{V}\)</span>, so the matrix
<span class="math notranslate nohighlight">\(2(\mathbf{X} - \mathbf{U}\mathbf{V})\)</span> is <span class="math notranslate nohighlight">\(n \times m\)</span>. On
the other hand <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> is <span class="math notranslate nohighlight">\(n \times r\)</span>, and we cannot
multiply a <span class="math notranslate nohighlight">\(n \times m\)</span> and a <span class="math notranslate nohighlight">\(n \times r\)</span> matrix since the
dimensions do not match!</p>
<p>We want to get <span class="math notranslate nohighlight">\(\frac{d}{d\mathbf{V}}\)</span>, which is the same shape of
<span class="math notranslate nohighlight">\(\mathbf{V}\)</span>, which is <span class="math notranslate nohighlight">\(r \times m\)</span>. So somehow we need to
take a <span class="math notranslate nohighlight">\(n \times m\)</span> matrix and a <span class="math notranslate nohighlight">\(n \times r\)</span> matrix,
multiply them together (perhaps with some transposes) to get a
<span class="math notranslate nohighlight">\(r \times m\)</span>. We can do this by multiplying <span class="math notranslate nohighlight">\(U^\top\)</span> by
<span class="math notranslate nohighlight">\((\mathbf{X} - \mathbf{U}\mathbf{V})\)</span>. Thus, we can guess the
solution to <a class="reference internal" href="#equation-eq-mat-goal-2">()</a> is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-45">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-45" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= 2\mathbf{U}^\top(\mathbf{X} - \mathbf{U}\mathbf{V}).\]</div>
<p>To show we that this works, we would be remiss to not provide a detailed
computation. If we already believe that this rule-of-thumb works, feel
free to skip past this derivation. To compute</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-46">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-46" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^2,\]</div>
<p>we must find for every <span class="math notranslate nohighlight">\(a\)</span>, and <span class="math notranslate nohighlight">\(b\)</span></p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-47">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-47" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= \frac{d}{dv_{ab}} \sum_{i, j}\left(x_{ij} - \sum_k u_{ik}v_{kj}\right)^2.\]</div>
<p>Recalling that all entries of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>
are constants as far as <span class="math notranslate nohighlight">\(\frac{d}{dv_{ab}}\)</span> is concerned, we may
push the derivative inside the sum, and apply the chain rule to the
square to get</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-48">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-48" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= \sum_{i, j}2\left(x_{ij} - \sum_k u_{ik}v_{kj}\right)\left(\sum_k u_{ik}\frac{dv_{kj}}{dv_{ab}} \right).\]</div>
<p>As in the previous derivation, we may note that
<span class="math notranslate nohighlight">\(\frac{dv_{kj}}{dv_{ab}}\)</span> is only non-zero if the <span class="math notranslate nohighlight">\(k=a\)</span> and
<span class="math notranslate nohighlight">\(j=b\)</span>. If either of those conditions do not hold, the term in the
sum is zero, and we may freely discard it. We see that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-49">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-49" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= \sum_{i}2\left(x_{ib} - \sum_k u_{ik}v_{kb}\right)u_{ia}.\]</div>
<p>An important subtlety here is that the requirement that <span class="math notranslate nohighlight">\(k=a\)</span> does
not occur inside the inner sum since that <span class="math notranslate nohighlight">\(k\)</span> is a dummy variable
which we are summing over inside the inner term. For a notationally
cleaner example, consider why</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-50">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-50" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx_1} \left(\sum_i x_i \right)^{2}= 2\left(\sum_i x_i \right).\]</div>
<p>From this point, we may start identifying components of the sum. First,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-51">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-51" title="Permalink to this equation">¶</a></span>\[\sum_k u_{ik}v_{kb} = [\mathbf{U}\mathbf{V}]_{ib}.\]</div>
<p>So the entire expression in the inside of the sum is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-52">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-52" title="Permalink to this equation">¶</a></span>\[x_{ib} - \sum_k u_{ik}v_{kb} = [\mathbf{X}-\mathbf{U}\mathbf{V}]_{ib}.\]</div>
<p>This means we may now write our derivative as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-53">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-53" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= 2\sum_{i}[\mathbf{X}-\mathbf{U}\mathbf{V}]_{ib}u_{ia}.\]</div>
<p>We want this to look like the <span class="math notranslate nohighlight">\(a, b\)</span> element of a matrix so we can
use the technique as in the previous example to arrive at a matrix
expression, which means that we need to exchange the order of the
indices on <span class="math notranslate nohighlight">\(u_{ia}\)</span>. If we notice that
<span class="math notranslate nohighlight">\(u_{ia} = [\mathbf{U}^\top]_{ai}\)</span>, we can then write</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-54">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-54" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= 2\sum_{i} [\mathbf{U}^\top]_{ai}[\mathbf{X}-\mathbf{U}\mathbf{V}]_{ib}.\]</div>
<p>This is a matrix product, and thus we can conclude that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-55">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-55" title="Permalink to this equation">¶</a></span>\[\frac{d}{dv_{ab}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= [2\mathbf{U}^\top(\mathbf{X}-\mathbf{U}\mathbf{V})]_{ab}.\]</div>
<p>and thus we may write the solution to <a class="reference internal" href="#equation-eq-mat-goal-2">()</a></p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-56">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-multivariable-calculus-56" title="Permalink to this equation">¶</a></span>\[\frac{d}{d\mathbf{V}} \|\mathbf{X} - \mathbf{U}\mathbf{V}\|_2^{2}= 2\mathbf{U}^\top(\mathbf{X} - \mathbf{U}\mathbf{V}).\]</div>
<p>This matches the solution we guessed above!</p>
<p>It is reasonable to ask at this point, “Why can I not just write down
matrix versions of all the calculus rules I have learned? It is clear
this is still mechanical. Why do we not just get it over with!” And
indeed there are such rules and <a class="bibtex reference internal" href="../chapter_references/zreferences.html#petersen-pedersen-ea-2008" id="id1">[Petersen et al., 2008]</a>
provides an excellent summary. However, due to the plethora of ways
matrix operations can be combined compared to single values, there are
many more matrix derivative rules than single variable ones. It is often
the case that it is best to work with the indices, or leave it up to
automatic differentiation when appropriate.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>In higher dimensions, we can define gradients which serve the same
purpose as derivatives in one dimension. These allow us to see how a
multi-variable function changes when we make an arbitrary small
change to the inputs.</p></li>
<li><p>The backpropagation algorithm can be seen to be a method of
organizing the multi-variable chain rule to allow for the efficient
computation of many partial derivatives.</p></li>
<li><p>Matrix calculus allows us to write the derivatives of matrix
expressions in concise ways.</p></li>
</ul>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Given a row vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, compute the
derivatives of both
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \boldsymbol{\beta}\mathbf{x}\)</span> and
<span class="math notranslate nohighlight">\(g(\mathbf{x}) = \mathbf{x}^\top\boldsymbol{\beta}^\top\)</span>. Why
do you get the same answer?</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> be an <span class="math notranslate nohighlight">\(n\)</span> dimension vector. What is
<span class="math notranslate nohighlight">\(\frac{\partial}{\partial\mathbf{v}}\|\mathbf{v}\|_2\)</span>?</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(L(x, y) = \log(e^x + e^y)\)</span>. Compute the gradient. What is
the sum of the components of the gradient?</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(f(x, y) = x^2y + xy^2\)</span>. Show that the only critical point
is <span class="math notranslate nohighlight">\((0,0)\)</span>. By considering <span class="math notranslate nohighlight">\(f(x, x)\)</span>, determine if
<span class="math notranslate nohighlight">\((0,0)\)</span> is a maximum, minimum, or neither.</p></li>
<li><p>Suppose that we are minimizing a function
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = g(\mathbf{x}) + h(\mathbf{x})\)</span>. How can we
geometrically interpret the condition of <span class="math notranslate nohighlight">\(\nabla f = 0\)</span> in
terms of <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span>?</p></li>
</ol>
</div>
<div class="section" id="discussions">
<h2><a class="reference external" href="https://discuss.mxnet.io/t/5150">Discussions</a><a class="headerlink" href="#discussions" title="Permalink to this headline">¶</a></h2>
<p><img alt="image0" src="../_images/qr_multivariable-calculus.svg" /></p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">Multivariable Calculus</a><ul>
<li><a class="reference internal" href="#higher-dimensional-differentiation">Higher-Dimensional Differentiation</a></li>
<li><a class="reference internal" href="#geometry-of-gradients-and-gradient-descent">Geometry of Gradients and Gradient Descent</a></li>
<li><a class="reference internal" href="#a-note-on-mathematical-optimization">A Note on Mathematical Optimization</a></li>
<li><a class="reference internal" href="#multivariate-chain-rule">Multivariate Chain Rule</a></li>
<li><a class="reference internal" href="#the-backpropagation-algorithm">The Backpropagation Algorithm</a></li>
<li><a class="reference internal" href="#hessians">Hessians</a></li>
<li><a class="reference internal" href="#a-little-matrix-calculus">A Little Matrix Calculus</a></li>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#exercises">Exercises</a></li>
<li><a class="reference internal" href="#discussions">Discussions</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
  </div>
        
        </main>
    </div>
  </body>
</html>