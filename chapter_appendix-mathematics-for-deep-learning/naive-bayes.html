<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>Naive Bayes &#8212; تعمّق في التعلّم العميق 0.7.1 documentation</title>

    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script type="text/javascript" src="../_static/d2l.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active">Naive Bayes</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_appendix-mathematics-for-deep-learning/naive-bayes.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai">
                  <i class="fas fa-user-graduate"></i>
                  Courses
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PDF
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.zip">
                  <i class="fas fa-download"></i>
                  All Notebooks
              </a>
          
              <a  class="mdl-navigation__link" href="https://discuss.mxnet.io">
                  <i class="fab fa-discourse"></i>
                  Discuss
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/d2l-ai/d2l-en">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh.d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  中文版
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="تعمّق في التعلّم العميق"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">تمهيد</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">الّرموز</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. مقدّمة</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. التمهيدات</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-math-and-keywords-translations/index.html">3. ملحق: ترجمة الألفاظ التقنية و الرياضية</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">المراجع</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="تعمّق في التعلّم العميق"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">تمهيد</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">الّرموز</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. مقدّمة</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. التمهيدات</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-math-and-keywords-translations/index.html">3. ملحق: ترجمة الألفاظ التقنية و الرياضية</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">المراجع</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="naive-bayes">
<span id="sec-naive-bayes"></span><h1>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">¶</a><a href="https://colab.research.google.com/github/['d2l-ai/d2l-en-colab']/blob/master/chapter_appendix-mathematics-for-deep-learning/naive-bayes.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/['d2l-ai/d2l-en-colab']/blob/master/chapter_appendix-mathematics-for-deep-learning/naive-bayes.ipynb'); return false;"> <button style="float:right", id="Colab" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab </button></a><div class="mdl-tooltip" data-mdl-for="Colab"> Open the notebook in Colab</div></h1>
<p>Throughout the previous sections, we learned about the theory of
probability and random variables. To put this theory to work, let’s
introduce the <em>naive Bayes</em> classifier. This uses nothing but
probabilistic fundamentals to allow us to perform classification of
digits.</p>
<p>Learning is all about making assumptions. If we want to classify a new
data point that we have never seen before we have to make some
assumptions about which data points are similar to each other. The naive
Bayes classifier, a popular and remarkably clear algorithm, assumes all
features are independent from each other to simplify the computation. In
this section, we will apply this model to recognize characters in
images.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">d2l</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">gluon</span><span class="p">,</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">use_svg_display</span><span class="p">()</span>
</pre></div>
</div>
<div class="section" id="optical-character-recognition">
<h2>Optical Character Recognition<a class="headerlink" href="#optical-character-recognition" title="Permalink to this headline">¶</a></h2>
<p>MNIST <a class="bibtex reference internal" href="../chapter_references/zreferences.html#lecun-bottou-bengio-ea-1998" id="id1">[LeCun et al., 1998]</a> is one of widely used
datasets. It contains 60,000 images for training and 10,000 images for
validation. Each image contains a handwritten digit from 0 to 9. The
task is classifying each image into the corresponding digit.</p>
<p>Gluon provides a <code class="docutils literal notranslate"><span class="pre">MNIST</span></code> class in the <code class="docutils literal notranslate"><span class="pre">data.vision</span></code> module to
automatically retrieve the dataset from the internet. Subsequently,
Gluon will use the already-downloaded local copy. We specify whether we
are requesting the training set or the test set by setting the value of
the parameter <code class="docutils literal notranslate"><span class="pre">train</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code>, respectively. Each
image is a grayscale image with both width and height of <span class="math notranslate nohighlight">\(28\)</span> with
shape (<span class="math notranslate nohighlight">\(28\)</span>,<span class="math notranslate nohighlight">\(28\)</span>,<span class="math notranslate nohighlight">\(1\)</span>). We use a customized
transformation to remove the last channel dimension. In addition, the
dataset represents each pixel by a unsigned <span class="math notranslate nohighlight">\(8\)</span>-bit integer. We
quantize them into binary features to simplify the problem.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span>

<span class="n">mnist_train</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">mnist_test</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</pre></div>
</div>
<p>We can access a particular example, which contains the image and the
corresponding label.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">mnist_train</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">array</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">))</span>
</pre></div>
</div>
<p>Our example, stored here in the variable <code class="docutils literal notranslate"><span class="pre">image</span></code>, corresponds to an
image with a height and width of <span class="math notranslate nohighlight">\(28\)</span> pixels.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">image</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>Our code stores the label of each image as a scalar. Its type is a
<span class="math notranslate nohighlight">\(32\)</span>-bit integer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">label</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">label</span><span class="p">),</span> <span class="n">label</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">),</span> <span class="n">mxnet</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;int32&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>We can also access multiple examples at the same time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">mnist_train</span><span class="p">[</span><span class="mi">10</span><span class="p">:</span><span class="mi">38</span><span class="p">]</span>
<span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="p">(</span><span class="mi">28</span><span class="p">,))</span>
</pre></div>
</div>
<p>Let’s visualize these examples.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">d2l</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">);</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_naive-bayes_362ea2_13_0.svg" src="../_images/output_naive-bayes_362ea2_13_0.svg" /></div>
</div>
<div class="section" id="the-probabilistic-model-for-classification">
<h2>The Probabilistic Model for Classification<a class="headerlink" href="#the-probabilistic-model-for-classification" title="Permalink to this headline">¶</a></h2>
<p>In a classification task, we map an example into a category. Here an
example is a grayscale <span class="math notranslate nohighlight">\(28\times 28\)</span> image, and a category is a
digit. (Refer to <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_softmax</span></code> for a more detailed
explanation.) One natural way to express the classification task is via
the probabilistic question: what is the most likely label given the
features (i.e., image pixels)? Denote by <span class="math notranslate nohighlight">\(\mathbf x\in\mathbb R^d\)</span>
the features of the example and <span class="math notranslate nohighlight">\(y\in\mathbb R\)</span> the label. Here
features are image pixels, where we can reshape a <span class="math notranslate nohighlight">\(2\)</span>-dimensional
image to a vector so that <span class="math notranslate nohighlight">\(d=28^2=784\)</span>, and labels are digits. The
probability of the label given the features is
<span class="math notranslate nohighlight">\(p(y \mid \mathbf{x})\)</span>. If we are able to compute these
probabilities, which are <span class="math notranslate nohighlight">\(p(y \mid \mathbf{x})\)</span> for
<span class="math notranslate nohighlight">\(y=0, \ldots,9\)</span> in our example, then the classifier will output
the prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> given by the expression:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-0">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-0" title="Permalink to this equation">¶</a></span>\[\hat{y} = \mathrm{argmax} \&gt; p(y  \mid  \mathbf{x}).\]</div>
<p>Unfortunately, this requires that we estimate
<span class="math notranslate nohighlight">\(p(y \mid \mathbf{x})\)</span> for every value of
<span class="math notranslate nohighlight">\(\mathbf{x} = x_1, ..., x_d\)</span>. Imagine that each feature could take
one of <span class="math notranslate nohighlight">\(2\)</span> values. For example, the feature <span class="math notranslate nohighlight">\(x_1 = 1\)</span> might
signify that the word apple appears in a given document and
<span class="math notranslate nohighlight">\(x_1 = 0\)</span> would signify that it does not. If we had <span class="math notranslate nohighlight">\(30\)</span>
such binary features, that would mean that we need to be prepared to
classify any of <span class="math notranslate nohighlight">\(2^{30}\)</span> (over 1 billion!) possible values of the
input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>Moreover, where is the learning? If we need to see every single possible
example in order to predict the corresponding label then we are not
really learning a pattern but just memorizing the dataset.</p>
</div>
<div class="section" id="the-naive-bayes-classifier">
<h2>The Naive Bayes Classifier<a class="headerlink" href="#the-naive-bayes-classifier" title="Permalink to this headline">¶</a></h2>
<p>Fortunately, by making some assumptions about conditional independence,
we can introduce some inductive bias and build a model capable of
generalizing from a comparatively modest selection of training examples.
To begin, let’s use Bayes theorem, to express the classifier as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-1">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-1" title="Permalink to this equation">¶</a></span>\[\hat{y} = \mathrm{argmax}_y \&gt; p(y  \mid  \mathbf{x}) = \mathrm{argmax}_y \&gt; \frac{p( \mathbf{x}  \mid  y) p(y)}{p(\mathbf{x})}.\]</div>
<p>Note that the denominator is the normalizing term <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span>
which does not depend on the value of the label <span class="math notranslate nohighlight">\(y\)</span>. As a result,
we only need to worry about comparing the numerator across different
values of <span class="math notranslate nohighlight">\(y\)</span>. Even if calculating the denominator turned out to
be intractable, we could get away with ignoring it, so long as we could
evaluate the numerator. Fortunately, even if we wanted to recover the
normalizing constant, we could. We can always recover the normalization
term since <span class="math notranslate nohighlight">\(\sum_y p(y \mid \mathbf{x}) = 1\)</span>.</p>
<p>Now, let’s focus on <span class="math notranslate nohighlight">\(p( \mathbf{x} \mid y)\)</span>. Using the chain rule
of probability, we can express the term <span class="math notranslate nohighlight">\(p( \mathbf{x} \mid y)\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-2">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-2" title="Permalink to this equation">¶</a></span>\[p(x_1  \mid y) \cdot p(x_2  \mid  x_1, y) \cdot ... \cdot p( x_d  \mid  x_1, ..., x_{d-1}, y).\]</div>
<p>By itself, this expression does not get us any further. We still must
estimate roughly <span class="math notranslate nohighlight">\(2^d\)</span> parameters. However, if we assume that <em>the
features are conditionally independent of each other, given the label</em>,
then suddenly we are in much better shape, as this term simplifies to
<span class="math notranslate nohighlight">\(\prod_i p(x_i \mid y)\)</span>, giving us the predictor</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-3">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-3" title="Permalink to this equation">¶</a></span>\[\hat{y} = \mathrm{argmax}_y \&gt; \prod_{i=1}^d p(x_i  \mid  y) p(y).\]</div>
<p>If we can estimate <span class="math notranslate nohighlight">\(\prod_i p(x_i=1 \mid y)\)</span> for every <span class="math notranslate nohighlight">\(i\)</span>
and <span class="math notranslate nohighlight">\(y\)</span>, and save its value in <span class="math notranslate nohighlight">\(P_{xy}[i, y]\)</span>, here
<span class="math notranslate nohighlight">\(P_{xy}\)</span> is a <span class="math notranslate nohighlight">\(d\times n\)</span> matrix with <span class="math notranslate nohighlight">\(n\)</span> being the
number of classes and <span class="math notranslate nohighlight">\(y\in\{1, \ldots, n\}\)</span>. In addition, we
estimate <span class="math notranslate nohighlight">\(p(y)\)</span> for every <span class="math notranslate nohighlight">\(y\)</span> and save it in <span class="math notranslate nohighlight">\(P_y[y]\)</span>,
with <span class="math notranslate nohighlight">\(P_y\)</span> a <span class="math notranslate nohighlight">\(n\)</span>-length vector. Then for any new example
<span class="math notranslate nohighlight">\(\mathbf x\)</span>, we could compute</p>
<div class="math notranslate nohighlight" id="equation-eq-naive-bayes-estimation">
<span class="eqno">()<a class="headerlink" href="#equation-eq-naive-bayes-estimation" title="Permalink to this equation">¶</a></span>\[\hat{y} = \mathrm{argmax}_y \&gt; \prod_{i=1}^d P_{xy}[x_i, y]P_y[y],\]</div>
<p>for any <span class="math notranslate nohighlight">\(y\)</span>. So our assumption of conditional independence has
taken the complexity of our model from an exponential dependence on the
number of features <span class="math notranslate nohighlight">\(\mathcal{O}(2^dn)\)</span> to a linear dependence,
which is <span class="math notranslate nohighlight">\(\mathcal{O}(dn)\)</span>.</p>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<p>The problem now is that we do not know <span class="math notranslate nohighlight">\(P_{xy}\)</span> and <span class="math notranslate nohighlight">\(P_y\)</span>.
So we need to estimate their values given some training data first. This
is <em>training</em> the model. Estimating <span class="math notranslate nohighlight">\(P_y\)</span> is not too hard. Since
we are only dealing with <span class="math notranslate nohighlight">\(10\)</span> classes, we may count the number of
occurrences <span class="math notranslate nohighlight">\(n_y\)</span> for each of the digits and divide it by the
total amount of data <span class="math notranslate nohighlight">\(n\)</span>. For instance, if digit 8 occurs
<span class="math notranslate nohighlight">\(n_8 = 5,800\)</span> times and we have a total of <span class="math notranslate nohighlight">\(n = 60,000\)</span>
images, the probability estimate is <span class="math notranslate nohighlight">\(p(y=8) = 0.0967\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">mnist_train</span><span class="p">[:]</span>  <span class="c1"># All training examples</span>

<span class="n">n_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">n_y</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">P_y</span> <span class="o">=</span> <span class="n">n_y</span> <span class="o">/</span> <span class="n">n_y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">P_y</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mf">0.09871667</span><span class="p">,</span> <span class="mf">0.11236667</span><span class="p">,</span> <span class="mf">0.0993</span>    <span class="p">,</span> <span class="mf">0.10218333</span><span class="p">,</span> <span class="mf">0.09736667</span><span class="p">,</span>
       <span class="mf">0.09035</span>   <span class="p">,</span> <span class="mf">0.09863333</span><span class="p">,</span> <span class="mf">0.10441667</span><span class="p">,</span> <span class="mf">0.09751666</span><span class="p">,</span> <span class="mf">0.09915</span>   <span class="p">])</span>
</pre></div>
</div>
<p>Now on to slightly more difficult things <span class="math notranslate nohighlight">\(P_{xy}\)</span>. Since we picked
black and white images, <span class="math notranslate nohighlight">\(p(x_i \mid y)\)</span> denotes the probability
that pixel <span class="math notranslate nohighlight">\(i\)</span> is switched on for class <span class="math notranslate nohighlight">\(y\)</span>. Just like
before we can go and count the number of times <span class="math notranslate nohighlight">\(n_{iy}\)</span> such that
an event occurs and divide it by the total number of occurrences of
<span class="math notranslate nohighlight">\(y\)</span>, i.e., <span class="math notranslate nohighlight">\(n_y\)</span>. But there is something slightly troubling:
certain pixels may never be black (e.g., for well cropped images the
corner pixels might always be white). A convenient way for statisticians
to deal with this problem is to add pseudo counts to all occurrences.
Hence, rather than <span class="math notranslate nohighlight">\(n_{iy}\)</span> we use <span class="math notranslate nohighlight">\(n_{iy}+1\)</span> and instead of
<span class="math notranslate nohighlight">\(n_y\)</span> we use <span class="math notranslate nohighlight">\(n_{y} + 1\)</span>. This is also called <em>Laplace
Smoothing</em>. It may seem ad-hoc, however it may be well motivated from a
Bayesian point-of-view.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">n_x</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()[</span><span class="n">Y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">==</span> <span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">P_xy</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span><span class="n">P_xy</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_naive-bayes_362ea2_17_0.svg" src="../_images/output_naive-bayes_362ea2_17_0.svg" /></div>
<p>By visualizing these <span class="math notranslate nohighlight">\(10\times 28\times 28\)</span> probabilities (for
each pixel for each class) we could get some mean looking digits.</p>
<p>Now we can use <a class="reference internal" href="#equation-eq-naive-bayes-estimation">()</a> to predict a new
image. Given <span class="math notranslate nohighlight">\(\mathbf x\)</span>, the following functions computes
<span class="math notranslate nohighlight">\(p(\mathbf x \mid y)p(y)\)</span> for every <span class="math notranslate nohighlight">\(y\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bayes_pred</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (28, 28) -&gt; (1, 28, 28)</span>
    <span class="n">p_xy</span> <span class="o">=</span> <span class="n">P_xy</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">P_xy</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">p_xy</span> <span class="o">=</span> <span class="n">p_xy</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># p(x|y)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p_xy</span><span class="p">)</span> <span class="o">*</span> <span class="n">P_y</span>

<span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">mnist_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">bayes_pred</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
</pre></div>
</div>
<p>This went horribly wrong! To find out why, let’s look at the per pixel
probabilities. They are typically numbers between <span class="math notranslate nohighlight">\(0.001\)</span> and
<span class="math notranslate nohighlight">\(1\)</span>. We are multiplying <span class="math notranslate nohighlight">\(784\)</span> of them. At this point it is
worth mentioning that we are calculating these numbers on a computer,
hence with a fixed range for the exponent. What happens is that we
experience <em>numerical underflow</em>, i.e., multiplying all the small
numbers leads to something even smaller until it is rounded down to
zero. We discussed this as a theoretical issue in
<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_maximum_likelihood</span></code>, but we see the phenomena clearly
here in practice.</p>
<p>As discussed in that section, we fix this by use the fact that
<span class="math notranslate nohighlight">\(\log a b = \log a + \log b\)</span>, i.e., we switch to summing
logarithms. Even if both <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are small numbers, the
logarithm values should be in a proper range.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;underflow:&#39;</span><span class="p">,</span> <span class="n">a</span><span class="o">**</span><span class="mi">784</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;logarithm is normal:&#39;</span><span class="p">,</span> <span class="mi">784</span><span class="o">*</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">underflow</span><span class="p">:</span> <span class="mf">0.0</span>
<span class="n">logarithm</span> <span class="ow">is</span> <span class="n">normal</span><span class="p">:</span> <span class="o">-</span><span class="mf">1805.2267129073316</span>
</pre></div>
</div>
<p>Since the logarithm is an increasing function, we can rewrite
<a class="reference internal" href="#equation-eq-naive-bayes-estimation">()</a> as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-4">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-naive-bayes-4" title="Permalink to this equation">¶</a></span>\[\hat{y} = \mathrm{argmax}_y \&gt; \sum_{i=1}^d \log P_{xy}[x_i, y] + \log P_y[y].\]</div>
<p>We can implement the following stable version:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">log_P_xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">P_xy</span><span class="p">)</span>
<span class="n">log_P_xy_neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">P_xy</span><span class="p">)</span>
<span class="n">log_P_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">P_y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">bayes_pred_stable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (28, 28) -&gt; (1, 28, 28)</span>
    <span class="n">p_xy</span> <span class="o">=</span> <span class="n">log_P_xy</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">log_P_xy_neg</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">p_xy</span> <span class="o">=</span> <span class="n">p_xy</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># p(x|y)</span>
    <span class="k">return</span> <span class="n">p_xy</span> <span class="o">+</span> <span class="n">log_P_y</span>

<span class="n">py</span> <span class="o">=</span> <span class="n">bayes_pred_stable</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">py</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">269.0042</span> <span class="p">,</span> <span class="o">-</span><span class="mf">301.73447</span><span class="p">,</span> <span class="o">-</span><span class="mf">245.21458</span><span class="p">,</span> <span class="o">-</span><span class="mf">218.8941</span> <span class="p">,</span> <span class="o">-</span><span class="mf">193.46907</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">206.10315</span><span class="p">,</span> <span class="o">-</span><span class="mf">292.54315</span><span class="p">,</span> <span class="o">-</span><span class="mf">114.62834</span><span class="p">,</span> <span class="o">-</span><span class="mf">220.35619</span><span class="p">,</span> <span class="o">-</span><span class="mf">163.18881</span><span class="p">])</span>
</pre></div>
</div>
<p>We may now check if the prediction is correct.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert label which is a scalar tensor of int32 dtype</span>
<span class="c1"># to a Python scalar integer for comparison</span>
<span class="n">py</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>If we now predict a few validation examples, we can see the Bayes
classifier works pretty well.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">bayes_pred_stable</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mnist_test</span><span class="p">[:</span><span class="mi">18</span><span class="p">]</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">preds</span><span class="p">]);</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_naive-bayes_362ea2_27_0.svg" src="../_images/output_naive-bayes_362ea2_27_0.svg" /></div>
<p>Finally, let’s compute the overall accuracy of the classifier.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mnist_test</span><span class="p">[:]</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="nb">float</span><span class="p">((</span><span class="n">preds</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Validation accuracy</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.8426</span>
</pre></div>
</div>
<p>Modern deep networks achieve error rates of less than <span class="math notranslate nohighlight">\(0.01\)</span>. The
relatively poor performance is due to the incorrect statistical
assumptions that we made in our model: we assumed that each and every
pixel are <em>independently</em> generated, depending only on the label. This
is clearly not how humans write digits, and this wrong assumption led to
the downfall of our overly naive (Bayes) classifier.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Using Bayes’ rule, a classifier can be made by assuming all observed
features are independent.</p></li>
<li><p>This classifier can be trained on a dataset by counting the number of
occurrences of combinations of labels and pixel values.</p></li>
<li><p>This classifier was the gold standard for decades for tasks such as
spam detection.</p></li>
</ul>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Consider the dataset <span class="math notranslate nohighlight">\([[0,0], [0,1], [1,0], [1,1]]\)</span> with labels
given by the XOR of the two elements <span class="math notranslate nohighlight">\([0,1,1,0]\)</span>. What are the
probabilities for a Naive Bayes classifier built on this dataset.
Does it successfully classify our points? If not, what assumptions
are violated?</p></li>
<li><p>Suppose that we did not use Laplace smoothing when estimating
probabilities and a data point arrived at testing time which
contained a value never observed in training. What would the model
output?</p></li>
<li><p>The naive Bayes classifier is a specific example of a Bayesian
network, where the dependence of random variables are encoded with a
graph structure. While the full theory is beyond the scope of this
section (see <a class="bibtex reference internal" href="../chapter_references/zreferences.html#koller-friedman-2009" id="id2">[Koller &amp; Friedman, 2009]</a> for full details),
explain why allowing explicit dependence between the two input
variables in the XOR model allows for the creation of a successful
classifier.</p></li>
</ol>
</div>
<div class="section" id="discussions">
<h2><a class="reference external" href="https://discuss.mxnet.io/t/5155">Discussions</a><a class="headerlink" href="#discussions" title="Permalink to this headline">¶</a></h2>
<p><img alt="image0" src="../_images/qr_naive-bayes.svg" /></p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">Naive Bayes</a><ul>
<li><a class="reference internal" href="#optical-character-recognition">Optical Character Recognition</a></li>
<li><a class="reference internal" href="#the-probabilistic-model-for-classification">The Probabilistic Model for Classification</a></li>
<li><a class="reference internal" href="#the-naive-bayes-classifier">The Naive Bayes Classifier</a></li>
<li><a class="reference internal" href="#training">Training</a></li>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#exercises">Exercises</a></li>
<li><a class="reference internal" href="#discussions">Discussions</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
  </div>
        
        </main>
    </div>
  </body>
</html>