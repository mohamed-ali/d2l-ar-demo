<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>2.3. Linear Algebra &#8212; تعمّق في التعلّم العميق 0.7.1 documentation</title>

    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script type="text/javascript" src="../_static/d2l.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2.4. Calculus" href="calculus.html" />
    <link rel="prev" title="2.2. Data Preprocessing" href="pandas.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">2. </span>التمهيدات</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">2.3. </span>Linear Algebra</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_preliminaries/linear-algebra.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai">
                  <i class="fas fa-user-graduate"></i>
                  Courses
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PDF
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.zip">
                  <i class="fas fa-download"></i>
                  All Notebooks
              </a>
          
              <a  class="mdl-navigation__link" href="https://discuss.mxnet.io">
                  <i class="fab fa-discourse"></i>
                  Discuss
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/d2l-ai/d2l-en">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh.d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  中文版
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="تعمّق في التعلّم العميق"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">تمهيد</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">الّرموز</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. مقدّمة</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">2. التمهيدات</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ndarray.html">2.1. معالجة البيانات</a></li>
<li class="toctree-l2"><a class="reference internal" href="pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="probability.html">2.6. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-math-and-keywords-translations/index.html">3. ملحق: ترجمة الألفاظ التقنية و الرياضية</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">المراجع</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="تعمّق في التعلّم العميق"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">تمهيد</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">الّرموز</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. مقدّمة</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">2. التمهيدات</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ndarray.html">2.1. معالجة البيانات</a></li>
<li class="toctree-l2"><a class="reference internal" href="pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="probability.html">2.6. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-math-and-keywords-translations/index.html">3. ملحق: ترجمة الألفاظ التقنية و الرياضية</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">المراجع</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="linear-algebra">
<span id="sec-linear-algebra"></span><h1><span class="section-number">2.3. </span>Linear Algebra<a class="headerlink" href="#linear-algebra" title="Permalink to this headline">¶</a><a href="https://colab.research.google.com/github/['d2l-ai/d2l-en-colab']/blob/master/chapter_preliminaries/linear-algebra.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/['d2l-ai/d2l-en-colab']/blob/master/chapter_preliminaries/linear-algebra.ipynb'); return false;"> <button style="float:right", id="Colab" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab </button></a><div class="mdl-tooltip" data-mdl-for="Colab"> Open the notebook in Colab</div></h1>
<p>Now that you can store and manipulate data, let’s briefly review the
subset of basic linear algebra that you will need to understand and
implement most of models covered in this book. Below, we introduce the
basic mathematical objects, arithmetic, and operations in linear
algebra, expressing each of them through mathematical notation and the
corresponding implementation in code.</p>
<div class="section" id="scalars">
<h2><span class="section-number">2.3.1. </span>Scalars<a class="headerlink" href="#scalars" title="Permalink to this headline">¶</a></h2>
<p>If you never studied linear algebra or machine learning, then your past
experience with math probably consisted of thinking about one number at
a time. And, if you ever balanced a checkbook or even paid for dinner at
a restaurant then you already know how to do basic things like adding
and multiplying pairs of numbers. For example, the temperature in Palo
Alto is <span class="math notranslate nohighlight">\(52\)</span> degrees Fahrenheit. Formally, we call values
consisting of just one numerical quantity <em>scalars</em>. If you wanted to
convert this value to Celsius (the metric system’s more sensible
temperature scale), you would evaluate the expression
<span class="math notranslate nohighlight">\(c = \frac{5}{9}(f - 32)\)</span>, setting <span class="math notranslate nohighlight">\(f\)</span> to <span class="math notranslate nohighlight">\(52\)</span>. In
this equation, each of the terms—<span class="math notranslate nohighlight">\(5\)</span>, <span class="math notranslate nohighlight">\(9\)</span>, and
<span class="math notranslate nohighlight">\(32\)</span>—are scalar values. The placeholders <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(f\)</span>
are called <em>variables</em> and they represent unknown scalar values.</p>
<p>In this book, we adopt the mathematical notation where scalar variables
are denoted by ordinary lower-cased letters (e.g., <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y\)</span>,
and <span class="math notranslate nohighlight">\(z\)</span>). We denote the space of all (continuous) <em>real-valued</em>
scalars by <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. For expedience, we will punt on rigorous
definitions of what precisely <em>space</em> is, but just remember for now that
the expression <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> is a formal way to say that
<span class="math notranslate nohighlight">\(x\)</span> is a real-valued scalar. The symbol <span class="math notranslate nohighlight">\(\in\)</span> can be
pronounced “in” and simply denotes membership in a set. Analogously, we
could write <span class="math notranslate nohighlight">\(x, y \in \{0, 1\}\)</span> to state that <span class="math notranslate nohighlight">\(x\)</span> and
<span class="math notranslate nohighlight">\(y\)</span> are numbers whose value can only be <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>In MXNet code, a scalar is represented by an <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> with just one
element. In the next snippet, we instantiate two scalars and perform
some familiar arithmetic operations with them, namely addition,
multiplication, division, and exponentiation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>

<span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">**</span> <span class="n">y</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="mf">5.</span><span class="p">),</span> <span class="n">array</span><span class="p">(</span><span class="mf">6.</span><span class="p">),</span> <span class="n">array</span><span class="p">(</span><span class="mf">1.5</span><span class="p">),</span> <span class="n">array</span><span class="p">(</span><span class="mf">9.</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="vectors">
<h2><span class="section-number">2.3.2. </span>Vectors<a class="headerlink" href="#vectors" title="Permalink to this headline">¶</a></h2>
<p>You can think of a vector as simply a list of scalar values. We call
these values the <em>elements</em> (<em>entries</em> or <em>components</em>) of the vector.
When our vectors represent examples from our dataset, their values hold
some real-world significance. For example, if we were training a model
to predict the risk that a loan defaults, we might associate each
applicant with a vector whose components correspond to their income,
length of employment, number of previous defaults, and other factors. If
we were studying the risk of heart attacks hospital patients potentially
face, we might represent each patient by a vector whose components
capture their most recent vital signs, cholesterol levels, minutes of
exercise per day, etc. In math notation, we will usually denote vectors
as bold-faced, lower-cased letters (e.g., <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{z})\)</span>.</p>
<p>In MXNet, we work with vectors via <span class="math notranslate nohighlight">\(1\)</span>-dimensional <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>s.
In general <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>s can have arbitrary lengths, subject to the
memory limits of your machine.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">x</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
</pre></div>
</div>
<p>We can refer to any element of a vector by using a subscript. For
example, we can refer to the <span class="math notranslate nohighlight">\(i^\mathrm{th}\)</span> element of
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> by <span class="math notranslate nohighlight">\(x_i\)</span>. Note that the element <span class="math notranslate nohighlight">\(x_i\)</span> is
a scalar, so we do not bold-face the font when referring to it.
Extensive literature considers column vectors to be the default
orientation of vectors, so does this book. In math, a vector
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> can be written as</p>
<div class="math notranslate nohighlight" id="equation-eq-vec-def">
<span class="eqno">(2.3.1)<a class="headerlink" href="#equation-eq-vec-def" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{x} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span> are elements of the vector. In code, we
access any element by indexing into the <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="length-dimensionality-and-shape">
<h3><span class="section-number">2.3.2.1. </span>Length, Dimensionality, and Shape<a class="headerlink" href="#length-dimensionality-and-shape" title="Permalink to this headline">¶</a></h3>
<p>Let’s revisit some concepts from <a class="reference internal" href="ndarray.html#sec-ndarray"><span class="std std-numref">Section 2.1</span></a>. A vector is
just an array of numbers. And just as every array has a length, so does
every vector. In math notation, if we want to say that a vector
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> consists of <span class="math notranslate nohighlight">\(n\)</span> real-valued scalars, we can
express this as <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>. The length of a
vector is commonly called the <em>dimension</em> of the vector.</p>
<p>As with an ordinary Python array, we can access the length of an
<code class="docutils literal notranslate"><span class="pre">ndarray</span></code> by calling Python’s built-in <code class="docutils literal notranslate"><span class="pre">len()</span></code> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">4</span>
</pre></div>
</div>
<p>When an <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> represents a vector (with precisely one axis), we
can also access its length via the <code class="docutils literal notranslate"><span class="pre">.shape</span></code> attribute. The shape is a
tuple that lists the length (dimensionality) along each axis of the
<code class="docutils literal notranslate"><span class="pre">ndarray</span></code>. For <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>s with just one axis, the shape has just
one element.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">4</span><span class="p">,)</span>
</pre></div>
</div>
<p>Note that the word “dimension” tends to get overloaded in these contexts
and this tends to confuse people. To clarify, we use the dimensionality
of a <em>vector</em> or an <em>axis</em> to refer to its length, i.e., the number of
elements of a vector or an axis. However, we use the dimensionality of
an <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> to refer to the number of axes that an <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> has.
In this sense, the dimensionality of some axis of an <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> will be
the length of that axis.</p>
</div>
</div>
<div class="section" id="matrices">
<h2><span class="section-number">2.3.3. </span>Matrices<a class="headerlink" href="#matrices" title="Permalink to this headline">¶</a></h2>
<p>Just as vectors generalize scalars from order <span class="math notranslate nohighlight">\(0\)</span> to order
<span class="math notranslate nohighlight">\(1\)</span>, matrices generalize vectors from order <span class="math notranslate nohighlight">\(1\)</span> to order
<span class="math notranslate nohighlight">\(2\)</span>. Matrices, which we will typically denote with bold-faced,
capital letters (e.g., <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>, and
<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>), are represented in code as <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>s with
<span class="math notranslate nohighlight">\(2\)</span> axes.</p>
<p>In math notation, we use <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>
to express that the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> consists of <span class="math notranslate nohighlight">\(m\)</span> rows
and <span class="math notranslate nohighlight">\(n\)</span> columns of real-valued scalars. Visually, we can
illustrate any matrix <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> as
a table, where each element <span class="math notranslate nohighlight">\(a_{ij}\)</span> belongs to the
<span class="math notranslate nohighlight">\(i^{\mathrm{th}}\)</span> row and <span class="math notranslate nohighlight">\(j^{\mathrm{th}}\)</span> column:</p>
<div class="math notranslate nohighlight" id="equation-eq-matrix-def">
<span class="eqno">(2.3.2)<a class="headerlink" href="#equation-eq-matrix-def" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{A}=\begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\ \end{bmatrix}.\end{split}\]</div>
<p>For any <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>, the shape of
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is (<span class="math notranslate nohighlight">\(m\)</span>, <span class="math notranslate nohighlight">\(n\)</span>) or <span class="math notranslate nohighlight">\(m \times n\)</span>.
Specifically, when a matrix has the same number of rows and columns, its
shape becomes a square; thus, it is called a <em>square matrix</em>.</p>
<p>We can create an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix in MXNet by specifying a
shape with two components <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(n\)</span> when calling any of
our favorite functions for instantiating an <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">A</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">8.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">12.</span><span class="p">,</span> <span class="mf">13.</span><span class="p">,</span> <span class="mf">14.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">16.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">,</span> <span class="mf">18.</span><span class="p">,</span> <span class="mf">19.</span><span class="p">]])</span>
</pre></div>
</div>
<p>We can access the scalar element <span class="math notranslate nohighlight">\(a_{ij}\)</span> of a matrix
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> in <a class="reference internal" href="#equation-eq-matrix-def">(2.3.2)</a> by specifying the indices
for the row (<span class="math notranslate nohighlight">\(i\)</span>) and column (<span class="math notranslate nohighlight">\(j\)</span>), such as
<span class="math notranslate nohighlight">\([\mathbf{A}]_{ij}\)</span>. When the scalar elements of a matrix
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, such as in <a class="reference internal" href="#equation-eq-matrix-def">(2.3.2)</a>, are not given,
we may simply use the lower-case letter of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
with the index subscript, <span class="math notranslate nohighlight">\(a_{ij}\)</span>, to refer to
<span class="math notranslate nohighlight">\([\mathbf{A}]_{ij}\)</span>. To keep notation simple, commas are inserted
to separate indices only when necessary, such as <span class="math notranslate nohighlight">\(a_{2, 3j}\)</span> and
<span class="math notranslate nohighlight">\([\mathbf{A}]_{2i-1, 3}\)</span>.</p>
<p>Sometimes, we want to flip the axes. When we exchange a matrix’s rows
and columns, the result is called the <em>transpose</em> of the matrix.
Formally, we signify a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>’s transpose by
<span class="math notranslate nohighlight">\(\mathbf{A}^\top\)</span> and if <span class="math notranslate nohighlight">\(\mathbf{B} = \mathbf{A}^\top\)</span>,
then <span class="math notranslate nohighlight">\(b_{ij} = a_{ji}\)</span> for any <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. Thus, the
transpose of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> in <a class="reference internal" href="#equation-eq-matrix-def">(2.3.2)</a> is a
<span class="math notranslate nohighlight">\(n \times m\)</span> matrix:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-0">
<span class="eqno">(2.3.3)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-0" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{A}^\top =
\begin{bmatrix}
    a_{11} &amp; a_{21} &amp; \dots  &amp; a_{m1} \\
    a_{12} &amp; a_{22} &amp; \dots  &amp; a_{m2} \\
    \vdots &amp; \vdots &amp; \ddots  &amp; \vdots \\
    a_{1n} &amp; a_{2n} &amp; \dots  &amp; a_{mn}
\end{bmatrix}.\end{split}\]</div>
<p>In code, we access a matrix’s transpose via the <code class="docutils literal notranslate"><span class="pre">T</span></code> attribute.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">,</span> <span class="mf">12.</span><span class="p">,</span> <span class="mf">16.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">,</span> <span class="mf">13.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">14.</span><span class="p">,</span> <span class="mf">18.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">3.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">,</span> <span class="mf">19.</span><span class="p">]])</span>
</pre></div>
</div>
<p>As a special type of the square matrix, a <em>symmetric matrix</em>
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is equal to its transpose:
<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{A}^\top\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="n">B</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">]])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">B</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span> <span class="kc">True</span><span class="p">,</span>  <span class="kc">True</span><span class="p">,</span>  <span class="kc">True</span><span class="p">],</span>
       <span class="p">[</span> <span class="kc">True</span><span class="p">,</span>  <span class="kc">True</span><span class="p">,</span>  <span class="kc">True</span><span class="p">],</span>
       <span class="p">[</span> <span class="kc">True</span><span class="p">,</span>  <span class="kc">True</span><span class="p">,</span>  <span class="kc">True</span><span class="p">]])</span>
</pre></div>
</div>
<p>Matrices are useful data structures: they allow us to organize data that
have different modalities of variation. For example, rows in our matrix
might correspond to different houses (data points), while columns might
correspond to different attributes. This should sound familiar if you
have ever used spreadsheet software or have read <a class="reference internal" href="pandas.html#sec-pandas"><span class="std std-numref">Section 2.2</span></a>.
Thus, although the default orientation of a single vector is a column
vector, in a matrix that represents a tabular dataset, it is more
conventional to treat each data point as a row vector in the matrix.
And, as we will see in later chapters, this convention will enable
common deep learning practices. For example, along the outermost axis of
an <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>, we can access or enumerate minibatches of data points,
or just data points if no minibatch exists.</p>
</div>
<div class="section" id="tensors">
<h2><span class="section-number">2.3.4. </span>Tensors<a class="headerlink" href="#tensors" title="Permalink to this headline">¶</a></h2>
<p>Just as vectors generalize scalars, and matrices generalize vectors, we
can build data structures with even more axes. Tensors give us a generic
way of describing <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>s with an arbitrary number of axes.
Vectors, for example, are first-order tensors, and matrices are
second-order tensors. Tensors are denoted with capital letters of a
special font face (e.g., <span class="math notranslate nohighlight">\(\mathsf{X}\)</span>, <span class="math notranslate nohighlight">\(\mathsf{Y}\)</span>, and
<span class="math notranslate nohighlight">\(\mathsf{Z}\)</span>) and their indexing mechanism (e.g., <span class="math notranslate nohighlight">\(x_{ijk}\)</span>
and <span class="math notranslate nohighlight">\([\mathsf{X}]_{1, 2i-1, 3}\)</span>) is similar to that of matrices.</p>
<p>Tensors will become more important when we start working with images,
which arrive as <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>s with 3 axes corresponding to the height,
width, and a <em>channel</em> axis for stacking the color channels (red, green,
and blue). For now, we will skip over higher order tensors and focus on
the basics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">X</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">8.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">]],</span>

       <span class="p">[[</span><span class="mf">12.</span><span class="p">,</span> <span class="mf">13.</span><span class="p">,</span> <span class="mf">14.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">16.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">,</span> <span class="mf">18.</span><span class="p">,</span> <span class="mf">19.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">20.</span><span class="p">,</span> <span class="mf">21.</span><span class="p">,</span> <span class="mf">22.</span><span class="p">,</span> <span class="mf">23.</span><span class="p">]]])</span>
</pre></div>
</div>
</div>
<div class="section" id="basic-properties-of-tensor-arithmetic">
<h2><span class="section-number">2.3.5. </span>Basic Properties of Tensor Arithmetic<a class="headerlink" href="#basic-properties-of-tensor-arithmetic" title="Permalink to this headline">¶</a></h2>
<p>Scalars, vectors, matrices, and tensors of an arbitrary number of axes
have some nice properties that often come in handy. For example, you
might have noticed from the definition of an elementwise operation that
any elementwise unary operation does not change the shape of its
operand. Similarly, given any two tensors with the same shape, the
result of any binary elementwise operation will be a tensor of that same
shape. For example, adding two matrices of the same shape performs
elementwise addition over these two matrices.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>  <span class="c1"># Assign a copy of A to B by allocating new memory</span>
<span class="n">A</span><span class="p">,</span> <span class="n">A</span> <span class="o">+</span> <span class="n">B</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">8.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">12.</span><span class="p">,</span> <span class="mf">13.</span><span class="p">,</span> <span class="mf">14.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">16.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">,</span> <span class="mf">18.</span><span class="p">,</span> <span class="mf">19.</span><span class="p">]]),</span> <span class="n">array</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">12.</span><span class="p">,</span> <span class="mf">14.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">16.</span><span class="p">,</span> <span class="mf">18.</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mf">22.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">24.</span><span class="p">,</span> <span class="mf">26.</span><span class="p">,</span> <span class="mf">28.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">32.</span><span class="p">,</span> <span class="mf">34.</span><span class="p">,</span> <span class="mf">36.</span><span class="p">,</span> <span class="mf">38.</span><span class="p">]]))</span>
</pre></div>
</div>
<p>Specifically, elementwise multiplication of two matrices is called their
<em>Hadamard product</em> (math notation <span class="math notranslate nohighlight">\(\odot\)</span>). Consider matrix
<span class="math notranslate nohighlight">\(\mathbf{B} \in \mathbb{R}^{m \times n}\)</span> whose element of row
<span class="math notranslate nohighlight">\(i\)</span> and column <span class="math notranslate nohighlight">\(j\)</span> is <span class="math notranslate nohighlight">\(b_{ij}\)</span>. The Hadamard product
of matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> (defined in <a class="reference internal" href="#equation-eq-matrix-def">(2.3.2)</a>) and
<span class="math notranslate nohighlight">\(\mathbf{B}\)</span></p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-1">
<span class="eqno">(2.3.4)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-1" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{A} \odot \mathbf{B} =
\begin{bmatrix}
    a_{11}  b_{11} &amp; a_{12}  b_{12} &amp; \dots  &amp; a_{1n}  b_{1n} \\
    a_{21}  b_{21} &amp; a_{22}  b_{22} &amp; \dots  &amp; a_{2n}  b_{2n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m1}  b_{m1} &amp; a_{m2}  b_{m2} &amp; \dots  &amp; a_{mn}  b_{mn}
\end{bmatrix}.\end{split}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">*</span> <span class="n">B</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span>  <span class="mf">0.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">4.</span><span class="p">,</span>   <span class="mf">9.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">16.</span><span class="p">,</span>  <span class="mf">25.</span><span class="p">,</span>  <span class="mf">36.</span><span class="p">,</span>  <span class="mf">49.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">64.</span><span class="p">,</span>  <span class="mf">81.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">121.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">144.</span><span class="p">,</span> <span class="mf">169.</span><span class="p">,</span> <span class="mf">196.</span><span class="p">,</span> <span class="mf">225.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">256.</span><span class="p">,</span> <span class="mf">289.</span><span class="p">,</span> <span class="mf">324.</span><span class="p">,</span> <span class="mf">361.</span><span class="p">]])</span>
</pre></div>
</div>
<p>Multiplying or adding a tensor by a scalar also does not change the
shape of the tensor, where each element of the operand tensor will be
added or multiplied by the scalar.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">a</span> <span class="o">+</span> <span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">([[[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">6.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">,</span> <span class="mf">12.</span><span class="p">,</span> <span class="mf">13.</span><span class="p">]],</span>

        <span class="p">[[</span><span class="mf">14.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">,</span> <span class="mf">16.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">18.</span><span class="p">,</span> <span class="mf">19.</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mf">21.</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">22.</span><span class="p">,</span> <span class="mf">23.</span><span class="p">,</span> <span class="mf">24.</span><span class="p">,</span> <span class="mf">25.</span><span class="p">]]]),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="reduction">
<h2><span class="section-number">2.3.6. </span>Reduction<a class="headerlink" href="#reduction" title="Permalink to this headline">¶</a></h2>
<p>One useful operation that we can perform with arbitrary tensors is to
calculate the sum of their elements. In mathematical notation, we
express sums using the <span class="math notranslate nohighlight">\(\sum\)</span> symbol. To express the sum of the
elements in a vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> of length <span class="math notranslate nohighlight">\(d\)</span>, we write
<span class="math notranslate nohighlight">\(\sum_{i=1}^d x_i\)</span>. In code, we can just call the <code class="docutils literal notranslate"><span class="pre">sum</span></code>
function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]),</span> <span class="n">array</span><span class="p">(</span><span class="mf">6.</span><span class="p">))</span>
</pre></div>
</div>
<p>We can express sums over the elements of tensors of arbitrary shape. For
example, the sum of the elements of an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> could be written
<span class="math notranslate nohighlight">\(\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">array</span><span class="p">(</span><span class="mf">190.</span><span class="p">))</span>
</pre></div>
</div>
<p>By default, invoking the <code class="docutils literal notranslate"><span class="pre">sum</span></code> function <em>reduces</em> a tensor along all
its axes to a scalar. We can also specify the axes along which the
tensor is reduced via summation. Take matrices as an example. To reduce
the row dimension (axis <span class="math notranslate nohighlight">\(0\)</span>) by summing up elements of all the
rows, we specify <code class="docutils literal notranslate"><span class="pre">axis=0</span></code> when invoking <code class="docutils literal notranslate"><span class="pre">sum</span></code>. Since the input
matrix reduces along axis <span class="math notranslate nohighlight">\(0\)</span> to generate the output vector, the
dimension of axis <span class="math notranslate nohighlight">\(0\)</span> of the input is lost in the output shape.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A_sum_axis0</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">A_sum_axis0</span><span class="p">,</span> <span class="n">A_sum_axis0</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mf">40.</span><span class="p">,</span> <span class="mf">45.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">55.</span><span class="p">]),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
</pre></div>
</div>
<p>Specifying <code class="docutils literal notranslate"><span class="pre">axis=1</span></code> will reduce the column dimension (axis <span class="math notranslate nohighlight">\(1\)</span>)
by summing up elements of all the columns. Thus, the dimension of axis
<span class="math notranslate nohighlight">\(1\)</span> of the input is lost in the output shape.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A_sum_axis1</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">A_sum_axis1</span><span class="p">,</span> <span class="n">A_sum_axis1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">([</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">22.</span><span class="p">,</span> <span class="mf">38.</span><span class="p">,</span> <span class="mf">54.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">]),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,))</span>
</pre></div>
</div>
<p>Reducing a matrix along both rows and columns via summation is
equivalent to summing up all the elements of the matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Same as A.sum()</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="mf">190.</span><span class="p">)</span>
</pre></div>
</div>
<p>A related quantity is the <em>mean</em>, which is also called the <em>average</em>. We
calculate the mean by dividing the sum by the total number of elements.
In code, we could just call <code class="docutils literal notranslate"><span class="pre">mean</span></code> on tensors of arbitrary shape.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">A</span><span class="o">.</span><span class="n">size</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="mf">9.5</span><span class="p">),</span> <span class="n">array</span><span class="p">(</span><span class="mf">9.5</span><span class="p">))</span>
</pre></div>
</div>
<p>Like <code class="docutils literal notranslate"><span class="pre">sum</span></code>, <code class="docutils literal notranslate"><span class="pre">mean</span></code> can also reduce a tensor along the specified
axes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">([</span> <span class="mf">8.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span> <span class="mf">8.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">]))</span>
</pre></div>
</div>
<div class="section" id="non-reduction-sum">
<h3><span class="section-number">2.3.6.1. </span>Non-Reduction Sum<a class="headerlink" href="#non-reduction-sum" title="Permalink to this headline">¶</a></h3>
<p>However, sometimes it can be useful to keep the number of axes unchanged
when invoking <code class="docutils literal notranslate"><span class="pre">sum</span></code> or <code class="docutils literal notranslate"><span class="pre">mean</span></code> by setting <code class="docutils literal notranslate"><span class="pre">keepdims=True</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sum_A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sum_A</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span> <span class="mf">6.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">22.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">38.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">54.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">70.</span><span class="p">]])</span>
</pre></div>
</div>
<p>For instance, since <code class="docutils literal notranslate"><span class="pre">sum_A</span></code> still keeps its <span class="math notranslate nohighlight">\(2\)</span> axes after
summing each row, we can divide <code class="docutils literal notranslate"><span class="pre">A</span></code> by <code class="docutils literal notranslate"><span class="pre">sum_A</span></code> with broadcasting.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">/</span> <span class="n">sum_A</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.16666667</span><span class="p">,</span> <span class="mf">0.33333334</span><span class="p">,</span> <span class="mf">0.5</span>       <span class="p">],</span>
       <span class="p">[</span><span class="mf">0.18181819</span><span class="p">,</span> <span class="mf">0.22727273</span><span class="p">,</span> <span class="mf">0.27272728</span><span class="p">,</span> <span class="mf">0.3181818</span> <span class="p">],</span>
       <span class="p">[</span><span class="mf">0.21052632</span><span class="p">,</span> <span class="mf">0.23684211</span><span class="p">,</span> <span class="mf">0.2631579</span> <span class="p">,</span> <span class="mf">0.28947368</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.22222222</span><span class="p">,</span> <span class="mf">0.24074075</span><span class="p">,</span> <span class="mf">0.25925925</span><span class="p">,</span> <span class="mf">0.2777778</span> <span class="p">],</span>
       <span class="p">[</span><span class="mf">0.22857143</span><span class="p">,</span> <span class="mf">0.24285714</span><span class="p">,</span> <span class="mf">0.25714287</span><span class="p">,</span> <span class="mf">0.27142859</span><span class="p">]])</span>
</pre></div>
</div>
<p>If we want to calculate the cumulative sum of elements of <code class="docutils literal notranslate"><span class="pre">A</span></code> along
some axis, say <code class="docutils literal notranslate"><span class="pre">axis=0</span></code> (row by row), we can call the <code class="docutils literal notranslate"><span class="pre">cumsum</span></code>
function. This function will not reduce the input tensor along any axis.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">12.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">,</span> <span class="mf">18.</span><span class="p">,</span> <span class="mf">21.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">24.</span><span class="p">,</span> <span class="mf">28.</span><span class="p">,</span> <span class="mf">32.</span><span class="p">,</span> <span class="mf">36.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">40.</span><span class="p">,</span> <span class="mf">45.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">55.</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="dot-products">
<h2><span class="section-number">2.3.7. </span>Dot Products<a class="headerlink" href="#dot-products" title="Permalink to this headline">¶</a></h2>
<p>So far, we have only performed elementwise operations, sums, and
averages. And if this was all we could do, linear algebra probably would
not deserve its own section. However, one of the most fundamental
operations is the dot product. Given two vectors
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^d\)</span>, their <em>dot product</em>
<span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{y}\)</span> (or
<span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y} \rangle\)</span>) is a sum over the
products of the elements at the same position:
<span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]),</span> <span class="n">array</span><span class="p">(</span><span class="mf">6.</span><span class="p">))</span>
</pre></div>
</div>
<p>Note that we can express the dot product of two vectors equivalently by
performing an elementwise multiplication and then a sum:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="mf">6.</span><span class="p">)</span>
</pre></div>
</div>
<p>Dot products are useful in a wide range of contexts. For example, given
some set of values, denoted by a vector
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> and a set of weights denoted by
<span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^d\)</span>, the weighted sum of the values in
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> according to the weights <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> could be
expressed as the dot product <span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{w}\)</span>. When
the weights are non-negative and sum to one (i.e.,
<span class="math notranslate nohighlight">\(\left(\sum_{i=1}^{d} {w_i} = 1\right)\)</span>), the dot product
expresses a <em>weighted average</em>. After normalizing two vectors to have
the unit length, the dot products express the cosine of the angle
between them. We will formally introduce this notion of <em>length</em> later
in this section.</p>
</div>
<div class="section" id="matrix-vector-products">
<h2><span class="section-number">2.3.8. </span>Matrix-Vector Products<a class="headerlink" href="#matrix-vector-products" title="Permalink to this headline">¶</a></h2>
<p>Now that we know how to calculate dot products, we can begin to
understand <em>matrix-vector products</em>. Recall the matrix
<span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> and the vector
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span> defined and visualized in
<a class="reference internal" href="#equation-eq-matrix-def">(2.3.2)</a> and <a class="reference internal" href="#equation-eq-vec-def">(2.3.1)</a> respectively. Let’s
start off by visualizing the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> in terms of its
row vectors</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-2">
<span class="eqno">(2.3.5)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-2" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix},\end{split}\]</div>
<p>where each <span class="math notranslate nohighlight">\(\mathbf{a}^\top_{i} \in \mathbb{R}^n\)</span> is a row vector
representing the <span class="math notranslate nohighlight">\(i^\mathrm{th}\)</span> row of the matrix
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. The matrix-vector product
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x}\)</span> is simply a column vector of length
<span class="math notranslate nohighlight">\(m\)</span>, whose <span class="math notranslate nohighlight">\(i^\mathrm{th}\)</span> element is the dot product
<span class="math notranslate nohighlight">\(\mathbf{a}^\top_i \mathbf{x}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-3">
<span class="eqno">(2.3.6)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-3" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{A}\mathbf{x}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix}\mathbf{x}
= \begin{bmatrix}
 \mathbf{a}^\top_{1} \mathbf{x}  \\
 \mathbf{a}^\top_{2} \mathbf{x} \\
\vdots\\
 \mathbf{a}^\top_{m} \mathbf{x}\\
\end{bmatrix}.\end{split}\]</div>
<p>We can think of multiplication by a matrix
<span class="math notranslate nohighlight">\(\mathbf{A}\in \mathbb{R}^{m \times n}\)</span> as a transformation that
projects vectors from <span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}^{m}\)</span>.
These transformations turn out to be remarkably useful. For example, we
can represent rotations as multiplications by a square matrix. As we
will see in subsequent chapters, we can also use matrix-vector products
to describe the most intensive calculations required when computing each
layer in a neural network given the values of the previous layer.</p>
<p>Expressing matrix-vector products in code with <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>s, we use
the same <code class="docutils literal notranslate"><span class="pre">dot</span></code> function as for dot products. When we call
<code class="docutils literal notranslate"><span class="pre">np.dot(A,</span> <span class="pre">x)</span></code> with a matrix <code class="docutils literal notranslate"><span class="pre">A</span></code> and a vector <code class="docutils literal notranslate"><span class="pre">x</span></code>, the
matrix-vector product is performed. Note that the column dimension of
<code class="docutils literal notranslate"><span class="pre">A</span></code> (its length along axis <span class="math notranslate nohighlight">\(1\)</span>) must be the same as the
dimension of <code class="docutils literal notranslate"><span class="pre">x</span></code> (its length).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,),</span> <span class="n">array</span><span class="p">([</span> <span class="mf">14.</span><span class="p">,</span>  <span class="mf">38.</span><span class="p">,</span>  <span class="mf">62.</span><span class="p">,</span>  <span class="mf">86.</span><span class="p">,</span> <span class="mf">110.</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="section" id="matrix-matrix-multiplication">
<h2><span class="section-number">2.3.9. </span>Matrix-Matrix Multiplication<a class="headerlink" href="#matrix-matrix-multiplication" title="Permalink to this headline">¶</a></h2>
<p>If you have gotten the hang of dot products and matrix-vector products,
then <em>matrix-matrix multiplication</em> should be straightforward.</p>
<p>Say that we have two matrices
<span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times k}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{B} \in \mathbb{R}^{k \times m}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-4">
<span class="eqno">(2.3.7)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-4" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{A}=\begin{bmatrix}
 a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1k} \\
 a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nk} \\
\end{bmatrix},\quad
\mathbf{B}=\begin{bmatrix}
 b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1m} \\
 b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 b_{k1} &amp; b_{k2} &amp; \cdots &amp; b_{km} \\
\end{bmatrix}.\end{split}\]</div>
<p>Denote by <span class="math notranslate nohighlight">\(\mathbf{a}^\top_{i} \in \mathbb{R}^k\)</span> the row vector
representing the <span class="math notranslate nohighlight">\(i^\mathrm{th}\)</span> row of the matrix
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{b}_{j} \in \mathbb{R}^k\)</span> be
the column vector from the <span class="math notranslate nohighlight">\(j^\mathrm{th}\)</span> column of the matrix
<span class="math notranslate nohighlight">\(\mathbf{B}\)</span>. To produce the matrix product
<span class="math notranslate nohighlight">\(\mathbf{C} = \mathbf{A}\mathbf{B}\)</span>, it is easiest to think of
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> in terms of its row vectors and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> in
terms of its column vectors:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-5">
<span class="eqno">(2.3.8)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-5" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix},
\quad \mathbf{B}=\begin{bmatrix}
 \mathbf{b}_{1} &amp; \mathbf{b}_{2} &amp; \cdots &amp; \mathbf{b}_{m} \\
\end{bmatrix}.\end{split}\]</div>
<p>Then the matrix product <span class="math notranslate nohighlight">\(\mathbf{C} \in \mathbb{R}^{n \times m}\)</span>
is produced as we simply compute each element <span class="math notranslate nohighlight">\(c_{ij}\)</span> as the dot
product <span class="math notranslate nohighlight">\(\mathbf{a}^\top_i \mathbf{b}_j\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-6">
<span class="eqno">(2.3.9)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-6" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix}
\begin{bmatrix}
 \mathbf{b}_{1} &amp; \mathbf{b}_{2} &amp; \cdots &amp; \mathbf{b}_{m} \\
\end{bmatrix}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \mathbf{b}_1 &amp; \mathbf{a}^\top_{1}\mathbf{b}_2&amp; \cdots &amp; \mathbf{a}^\top_{1} \mathbf{b}_m \\
 \mathbf{a}^\top_{2}\mathbf{b}_1 &amp; \mathbf{a}^\top_{2} \mathbf{b}_2 &amp; \cdots &amp; \mathbf{a}^\top_{2} \mathbf{b}_m \\
 \vdots &amp; \vdots &amp; \ddots &amp;\vdots\\
\mathbf{a}^\top_{n} \mathbf{b}_1 &amp; \mathbf{a}^\top_{n}\mathbf{b}_2&amp; \cdots&amp; \mathbf{a}^\top_{n} \mathbf{b}_m
\end{bmatrix}.\end{split}\]</div>
<p>We can think of the matrix-matrix multiplication <span class="math notranslate nohighlight">\(\mathbf{AB}\)</span> as
simply performing <span class="math notranslate nohighlight">\(m\)</span> matrix-vector products and stitching the
results together to form an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix. Just as with
ordinary dot products and matrix-vector products, we can compute
matrix-matrix multiplication by using the <code class="docutils literal notranslate"><span class="pre">dot</span></code> function. In the
following snippet, we perform matrix multiplication on <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>.
Here, <code class="docutils literal notranslate"><span class="pre">A</span></code> is a matrix with <span class="math notranslate nohighlight">\(5\)</span> rows and <span class="math notranslate nohighlight">\(4\)</span> columns, and
<code class="docutils literal notranslate"><span class="pre">B</span></code> is a matrix with <span class="math notranslate nohighlight">\(4\)</span> rows and <span class="math notranslate nohighlight">\(3\)</span> columns. After
multiplication, we obtain a matrix with <span class="math notranslate nohighlight">\(5\)</span> rows and <span class="math notranslate nohighlight">\(3\)</span>
columns.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span> <span class="mf">6.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">22.</span><span class="p">,</span> <span class="mf">22.</span><span class="p">,</span> <span class="mf">22.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">38.</span><span class="p">,</span> <span class="mf">38.</span><span class="p">,</span> <span class="mf">38.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">54.</span><span class="p">,</span> <span class="mf">54.</span><span class="p">,</span> <span class="mf">54.</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">70.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">]])</span>
</pre></div>
</div>
<p>Matrix-matrix multiplication can be simply called <em>matrix
multiplication</em>, and should not be confused with the Hadamard product.</p>
</div>
<div class="section" id="norms">
<h2><span class="section-number">2.3.10. </span>Norms<a class="headerlink" href="#norms" title="Permalink to this headline">¶</a></h2>
<p>Some of the most useful operators in linear algebra are <em>norms</em>.
Informally, the norm of a vector tells us how <em>big</em> a vector is. The
notion of <em>size</em> under consideration here concerns not dimensionality
but rather the magnitude of the components.</p>
<p>In linear algebra, a vector norm is a function <span class="math notranslate nohighlight">\(f\)</span> that maps a
vector to a scalar, satisfying a handful of properties. Given any vector
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, the first property says that if we scale all the
elements of a vector by a constant factor <span class="math notranslate nohighlight">\(\alpha\)</span>, its norm also
scales by the <em>absolute value</em> of the same constant factor:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-7">
<span class="eqno">(2.3.10)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-7" title="Permalink to this equation">¶</a></span>\[f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).\]</div>
<p>The second property is the familiar triangle inequality:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-8">
<span class="eqno">(2.3.11)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-8" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).\]</div>
<p>The third property simply says that the norm must be non-negative:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-9">
<span class="eqno">(2.3.12)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-9" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x}) \geq 0.\]</div>
<p>That makes sense, as in most contexts the smallest <em>size</em> for anything
is 0. The final property requires that the smallest norm is achieved and
only achieved by a vector consisting of all zeros.</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-10">
<span class="eqno">(2.3.13)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-10" title="Permalink to this equation">¶</a></span>\[\forall i, [\mathbf{x}]_i = 0 \Leftrightarrow f(\mathbf{x})=0.\]</div>
<p>You might notice that norms sound a lot like measures of distance. And
if you remember Euclidean distances (think Pythagoras’ theorem) from
grade school, then the concepts of non-negativity and the triangle
inequality might ring a bell. In fact, the Euclidean distance is a norm:
specifically it is the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm. Suppose that the elements in
the <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are
<span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>. The <span class="math notranslate nohighlight">\(\ell_2\)</span> <em>norm</em> of
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is the square root of the sum of the squares of the
vector elements:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-11">
<span class="eqno">(2.3.14)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-11" title="Permalink to this equation">¶</a></span>\[\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},\]</div>
<p>where the subscript <span class="math notranslate nohighlight">\(2\)</span> is often omitted in <span class="math notranslate nohighlight">\(\ell_2\)</span> norms,
i.e., <span class="math notranslate nohighlight">\(\|\mathbf{x}\|\)</span> is equivalent to <span class="math notranslate nohighlight">\(\|\mathbf{x}\|_2\)</span>.
In code, we can calculate the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm of a vector by calling
<code class="docutils literal notranslate"><span class="pre">linalg.norm</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="mf">5.</span><span class="p">)</span>
</pre></div>
</div>
<p>In deep learning, we work more often with the squared <span class="math notranslate nohighlight">\(\ell_2\)</span>
norm. You will also frequently encounter the <span class="math notranslate nohighlight">\(\ell_1\)</span> <em>norm</em>,
which is expressed as the sum of the absolute values of the vector
elements:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-12">
<span class="eqno">(2.3.15)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-12" title="Permalink to this equation">¶</a></span>\[\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.\]</div>
<p>As compared with the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm, it is less influenced by
outliers. To calculate the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm, we compose the absolute
value function with a sum over the elements.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">u</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="mf">7.</span><span class="p">)</span>
</pre></div>
</div>
<p>Both the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm and the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm are special
cases of the more general <span class="math notranslate nohighlight">\(\ell_p\)</span> <em>norm</em>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-13">
<span class="eqno">(2.3.16)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-13" title="Permalink to this equation">¶</a></span>\[\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.\]</div>
<p>Analogous to <span class="math notranslate nohighlight">\(\ell_2\)</span> norms of vectors, the <em>Frobenius norm</em> of a
matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{m \times n}\)</span> is the square root
of the sum of the squares of the matrix elements:</p>
<div class="math notranslate nohighlight" id="equation-chapter-preliminaries-linear-algebra-14">
<span class="eqno">(2.3.17)<a class="headerlink" href="#equation-chapter-preliminaries-linear-algebra-14" title="Permalink to this equation">¶</a></span>\[\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.\]</div>
<p>The Frobenius norm satisfies all the properties of vector norms. It
behaves as if it were an <span class="math notranslate nohighlight">\(\ell_2\)</span> norm of a matrix-shaped vector.
Invoking <code class="docutils literal notranslate"><span class="pre">linalg.norm</span></code> will calculate the Frobenius norm of a matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="mf">6.</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="norms-and-objectives">
<span id="subsec-norms-and-objectives"></span><h3><span class="section-number">2.3.10.1. </span>Norms and Objectives<a class="headerlink" href="#norms-and-objectives" title="Permalink to this headline">¶</a></h3>
<p>While we do not want to get too far ahead of ourselves, we can plant
some intuition already about why these concepts are useful. In deep
learning, we are often trying to solve optimization problems: <em>maximize</em>
the probability assigned to observed data; <em>minimize</em> the distance
between predictions and the ground-truth observations. Assign vector
representations to items (like words, products, or news articles) such
that the distance between similar items is minimized, and the distance
between dissimilar items is maximized. Oftentimes, the objectives,
perhaps the most important components of deep learning algorithms
(besides the data), are expressed as norms.</p>
</div>
</div>
<div class="section" id="more-on-linear-algebra">
<h2><span class="section-number">2.3.11. </span>More on Linear Algebra<a class="headerlink" href="#more-on-linear-algebra" title="Permalink to this headline">¶</a></h2>
<p>In just this section, we have taught you all the linear algebra that you
will need to understand a remarkable chunk of modern deep learning.
There is a lot more to linear algebra and a lot of that mathematics is
useful for machine learning. For example, matrices can be decomposed
into factors, and these decompositions can reveal low-dimensional
structure in real-world datasets. There are entire subfields of machine
learning that focus on using matrix decompositions and their
generalizations to high-order tensors to discover structure in datasets
and solve prediction problems. But this book focuses on deep learning.
And we believe you will be much more inclined to learn more mathematics
once you have gotten your hands dirty deploying useful machine learning
models on real datasets. So while we reserve the right to introduce more
mathematics much later on, we will wrap up this section here.</p>
<p>If you are eager to learn more about linear algebra, you may refer to
either <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_geometry-linear-algebric-ops</span></code> or other excellent
resources <a class="bibtex reference internal" href="../chapter_references/zreferences.html#strang-1993" id="id1">[Strang, 1993]</a><a class="bibtex reference internal" href="../chapter_references/zreferences.html#kolter-2008" id="id2">[Kolter, 2008]</a><a class="bibtex reference internal" href="../chapter_references/zreferences.html#petersen-pedersen-ea-2008" id="id3">[Petersen et al., 2008]</a>.</p>
</div>
<div class="section" id="summary">
<h2><span class="section-number">2.3.12. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Scalars, vectors, matrices, and tensors are basic mathematical
objects in linear algebra.</p></li>
<li><p>Vectors generalize scalars, and matrices generalize vectors.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> representation, scalars, vectors, matrices, and
tensors have 0, 1, 2, and an arbitrary number of axes, respectively.</p></li>
<li><p>A tensor can be reduced along the specified axes by <code class="docutils literal notranslate"><span class="pre">sum</span></code> and
<code class="docutils literal notranslate"><span class="pre">mean</span></code>.</p></li>
<li><p>Elementwise multiplication of two matrices is called their Hadamard
product. It is different from matrix multiplication.</p></li>
<li><p>In deep learning, we often work with norms such as the <span class="math notranslate nohighlight">\(\ell_1\)</span>
norm, the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm, and the Frobenius norm.</p></li>
<li><p>We can perform a variety of operations over scalars, vectors,
matrices, and tensors with <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> functions.</p></li>
</ul>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">2.3.13. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Prove that the transpose of a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>’s transpose
is <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>: <span class="math notranslate nohighlight">\((\mathbf{A}^\top)^\top = \mathbf{A}\)</span>.</p></li>
<li><p>Given two matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>, show
that the sum of transposes is equal to the transpose of a sum:
<span class="math notranslate nohighlight">\(\mathbf{A}^\top + \mathbf{B}^\top = (\mathbf{A} + \mathbf{B})^\top\)</span>.</p></li>
<li><p>Given any square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, is
<span class="math notranslate nohighlight">\(\mathbf{A} + \mathbf{A}^\top\)</span> always symmetric? Why?</p></li>
<li><p>We defined the tensor <code class="docutils literal notranslate"><span class="pre">X</span></code> of shape (<span class="math notranslate nohighlight">\(2\)</span>, <span class="math notranslate nohighlight">\(3\)</span>,
<span class="math notranslate nohighlight">\(4\)</span>) in this section. What is the output of <code class="docutils literal notranslate"><span class="pre">len(X)</span></code>?</p></li>
<li><p>For a tensor <code class="docutils literal notranslate"><span class="pre">X</span></code> of arbitrary shape, does <code class="docutils literal notranslate"><span class="pre">len(X)</span></code> always
correspond to the length of a certain axis of <code class="docutils literal notranslate"><span class="pre">X</span></code>? What is that
axis?</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">/</span> <span class="pre">A.sum(axis=1)</span></code> and see what happens. Can you analyze the
reason?</p></li>
<li><p>When traveling between two points in Manhattan, what is the distance
that you need to cover in terms of the coordinates, i.e., in terms of
avenues and streets? Can you travel diagonally?</p></li>
<li><p>Consider a tensor with shape (<span class="math notranslate nohighlight">\(2\)</span>, <span class="math notranslate nohighlight">\(3\)</span>, <span class="math notranslate nohighlight">\(4\)</span>). What
are the shapes of the summation outputs along axis <span class="math notranslate nohighlight">\(0\)</span>,
<span class="math notranslate nohighlight">\(1\)</span>, and <span class="math notranslate nohighlight">\(2\)</span>?</p></li>
<li><p>Feed a tensor with 3 or more axes to the <code class="docutils literal notranslate"><span class="pre">linalg.norm</span></code> function and
observe its output. What does this function compute for
<code class="docutils literal notranslate"><span class="pre">ndarray</span></code>s of arbitrary shape?</p></li>
</ol>
</div>
<div class="section" id="discussions">
<h2><span class="section-number">2.3.14. </span><a class="reference external" href="https://discuss.mxnet.io/t/2317">Discussions</a><a class="headerlink" href="#discussions" title="Permalink to this headline">¶</a></h2>
<p><img alt="image0" src="../_images/qr_linear-algebra.svg" /></p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">2.3. Linear Algebra</a><ul>
<li><a class="reference internal" href="#scalars">2.3.1. Scalars</a></li>
<li><a class="reference internal" href="#vectors">2.3.2. Vectors</a><ul>
<li><a class="reference internal" href="#length-dimensionality-and-shape">2.3.2.1. Length, Dimensionality, and Shape</a></li>
</ul>
</li>
<li><a class="reference internal" href="#matrices">2.3.3. Matrices</a></li>
<li><a class="reference internal" href="#tensors">2.3.4. Tensors</a></li>
<li><a class="reference internal" href="#basic-properties-of-tensor-arithmetic">2.3.5. Basic Properties of Tensor Arithmetic</a></li>
<li><a class="reference internal" href="#reduction">2.3.6. Reduction</a><ul>
<li><a class="reference internal" href="#non-reduction-sum">2.3.6.1. Non-Reduction Sum</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dot-products">2.3.7. Dot Products</a></li>
<li><a class="reference internal" href="#matrix-vector-products">2.3.8. Matrix-Vector Products</a></li>
<li><a class="reference internal" href="#matrix-matrix-multiplication">2.3.9. Matrix-Matrix Multiplication</a></li>
<li><a class="reference internal" href="#norms">2.3.10. Norms</a><ul>
<li><a class="reference internal" href="#norms-and-objectives">2.3.10.1. Norms and Objectives</a></li>
</ul>
</li>
<li><a class="reference internal" href="#more-on-linear-algebra">2.3.11. More on Linear Algebra</a></li>
<li><a class="reference internal" href="#summary">2.3.12. Summary</a></li>
<li><a class="reference internal" href="#exercises">2.3.13. Exercises</a></li>
<li><a class="reference internal" href="#discussions">2.3.14. Discussions</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="pandas.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>2.2. Data Preprocessing</div>
         </div>
     </a>
     <a id="button-next" href="calculus.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>2.4. Calculus</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>